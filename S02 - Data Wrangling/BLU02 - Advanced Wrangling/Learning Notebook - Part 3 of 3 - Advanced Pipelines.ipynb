{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU02 - Learning Notebook - Data wrangling workflows - Part 3 of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Advanced pipelines in scikit-learn\n",
    "\n",
    "Remember our workflow diagram? Let's look at it again.\n",
    "\n",
    "![data_transformation_workflow](./media/data_processing_workflow.png)\n",
    "\n",
    "*Fig 1. - A standard workflow (again).*\n",
    "\n",
    "Pandas, as amazing as it is, can only take us so far.\n",
    "\n",
    "There, beyond the known universe, lies **modeling**.\n",
    "\n",
    "Where we are at this point:\n",
    "* We are to perform transformations on data, setting up robust pipelines using nothing but Pandas\n",
    "* We can combine different dataframes, to enrich our datasets or generate new ones.\n",
    "\n",
    "Thus, here we are, modeling lying ahead of us. What's exactly new about modeling though?\n",
    "\n",
    "We will be using the same dataset, but this time we will create a train-test split, as we would do before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = pd.read_csv('./data/works.csv')\n",
    "X_train, X_test = train_test_split(works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 How is modeling different from transformation\n",
    "\n",
    "In Pandas, we merely transformed the original dataframe into a new one.\n",
    "\n",
    "But sometimes, this isn't possible. Let's start with an example: encoding categorical variables.\n",
    "\n",
    "Remember: we need to perform the same transformations on train and test data (and whatever data comes next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    df = df.copy()\n",
    "    df = (df.pipe(remove_intervals)\n",
    "            .pipe(label_encoder, 'ComposerName'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_intervals(df):\n",
    "    df = df.copy()\n",
    "    mask = df['Interval'].isnull()\n",
    "    df = (df.loc[mask, :]\n",
    "            .drop(columns='Interval'))\n",
    "    return df\n",
    "    \n",
    "\n",
    "def label_encoder(df, column):\n",
    "    df = df.copy()\n",
    "    df[column + 'Encoded'] = df[column].astype('category').cat.codes\n",
    "    return df\n",
    "\n",
    "\n",
    "X_train_ = transform_data(X_train)\n",
    "\n",
    "train_alban_berg = X_train_['ComposerName'] == 'Berg,  Alban'\n",
    "(X_train_.loc[train_alban_berg, ['ComposerName', 'ComposerNameEncoded']]\n",
    "         .drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All is good. We removed the intermissions (just like we did previously), and we transformed the original dataframe.\n",
    "\n",
    "For convenience, we are keeping only the `ComposerName` and `ComposerNameEncoded` columns and removing duplicates.\n",
    "\n",
    "Let's do the same to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = transform_data(X_test)\n",
    "\n",
    "test_alban_berg = X_test_['ComposerName'] == 'Berg,  Alban'\n",
    "(X_test_.loc[test_alban_berg, ['ComposerName', 'ComposerNameEncoded']]\n",
    "        .drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the problem? The same `ComposerName` can (and will, in all probability) get different encodings.\n",
    "\n",
    "This problem is significant, as it would lead us to make wrong predictions!!\n",
    "\n",
    "There are other cases in which problems arise. For instance, when replacing missing values with the mean:\n",
    "* You are supposed to compute the mean on the training set and use it to transform both train and test sets\n",
    "* Otherwise, you end up underestimating your correct generalization error.\n",
    "\n",
    "This particular unit is not about modeling at a conceptual level, but you get the point: \n",
    "* Somehow, you need to fit the transformer on your training data first (e.g., define the encodings, compute the means)\n",
    "* Transform both train and test sets (and any data that might come in, really) using the pre-fitted transformers.\n",
    "\n",
    "These transformations are more like modeling. In fact, all of this *is* modeling and part of your model. \n",
    "\n",
    "How do we solve this? **We need transformers that are more like models.**\n",
    "\n",
    "## 3.2 Meet the sklearn-like transformers\n",
    "\n",
    "There are at three fundamental verbs in scikit-learn and sklearn-like libraries:\n",
    "* `.fit()`\n",
    "* `.transform()`\n",
    "* `.predict()`.\n",
    "\n",
    "You are already familiar with `.fit()` and `.predict()`, from the Bootcamp and the Hackathon #1.  We use them to train models and make predictions.\n",
    "\n",
    "Here, we will explore a new combo: `.fit()` and `.transform()`. This is how it works.\n",
    "\n",
    "![sklearn_like_transformation_pipeline](./media/sklearn_like_transformation_pipeline.png)\n",
    "\n",
    "*Fig 2. - A data pipeline with consistent transformers, fitted on the training set.*\n",
    "\n",
    "In short, we fit a transformer on the training data and use to transform the training data.\n",
    "\n",
    "We will, however, return the transformer so we can use it to transform new, incoming data as well. Confusing? Perhaps.\n",
    "\n",
    "Time to get practical: meet the `categorical_encoders`, a set of transformers for encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder(cols=['ComposerName'])\n",
    "X_train_ = encoder.fit_transform(X_train)\n",
    "X_train_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use transform our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = encoder.transform(X_test)\n",
    "X_test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-do our transformer functions so that they can either fit a transformer or accept a pre-fitted one.\n",
    "\n",
    "We have to change our `label_encoder()` first to incorporate this logic. Then we need to adapt `transform_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, encoder=None):\n",
    "    df = df.copy()\n",
    "    df, encoder = (df.pipe(remove_intervals)\n",
    "                     .pipe(label_encoder, 'ComposerName', encoder))\n",
    "    \n",
    "    return df, encoder\n",
    "\n",
    "\n",
    "def label_encoder(df, columns, encoder=None):\n",
    "    if not encoder:\n",
    "        encoder = OrdinalEncoder(cols=[columns])\n",
    "        encoder.fit(df)\n",
    "        \n",
    "    preview_encodings(encoder)\n",
    "\n",
    "    df = df.copy()\n",
    "    df = encoder.transform(df)\n",
    "\n",
    "    return df, encoder\n",
    "\n",
    "    \n",
    "def preview_encodings(encoder):\n",
    "    encodings = encoder.category_mapping[0]['mapping'][:4]\n",
    "    print('Encodings: {}'.format(encodings))\n",
    "    return None\n",
    "\n",
    "X_train_, encoder = transform_data(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we changed our functions so that they can receive an encoder. \n",
    "\n",
    "Otherwise, they fit and return the new one for re-use.\n",
    "\n",
    "From a consistency standpoint, things should be looking good. Nonetheless, you are previewing the encoder as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = transform_data(X_test, encoder=encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of transformations do you need to perform this way? Some widespread ones are:\n",
    "* Encoding (as we've seen)\n",
    "* Scaling\n",
    "* Vectorization (you will learn about this in the next specialization!)\n",
    "* Missing data imputation.\n",
    "\n",
    "Now, this changes things (right?):\n",
    "* We lose the ability to do method chaining, as we have to return encodings as intermediate outputs\n",
    "* We need to segregate pipelines for training (fit and transform) and test (transform), which adds complexity and it's error-prone.\n",
    "\n",
    "Because we will perform the same transformations on all datasets, storing all the correct steps is critical for reproducibility and consistency.\n",
    "\n",
    "It turns out, scikit-learn provides us with a distinctive take on pipelines, to wrap all of this in a single META-TRANSFORMER.\n",
    "\n",
    "![megazord](./media/megazord.png)\n",
    "\n",
    "*Fig 3. - A meta-transformer in practice.*\n",
    "\n",
    "Meet the Megazord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pipelines\n",
    "\n",
    "The sklearn's `Pipelines` provide a higher level of abstraction than the individual building blocks.\n",
    "\n",
    "Let's tie together all these sequential transformers and run `Megazord.fit()` and `Megazord.transform()` on the whole thing.\n",
    "\n",
    "That would make managing our code much easier, right? Let's do it:\n",
    "* We want to replace the missing values with the mode\n",
    "* We want to one-hot-encode all categorical variables.\n",
    "\n",
    "First things first, some Pandas magic: let's drop the ID columns and exclude the intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    df = df.copy()\n",
    "    df = (df.pipe(drop_id_fields)\n",
    "            .pipe(remove_intervals)\n",
    "            .drop_duplicates())\n",
    "    return df\n",
    "\n",
    "def drop_id_fields(df):\n",
    "    columns = ['GUID', 'ProgramID', 'WorkID', 'MovementID']\n",
    "    df = df.copy()\n",
    "    df = df.drop(columns=columns)\n",
    "    return df\n",
    "\n",
    "X_train_ = prepare_data(X_train)\n",
    "X_train_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = prepare_data(X_test)\n",
    "X_test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we are going to do:\n",
    "1. Create an `Imputer` that replace missing values with the mode of the column\n",
    "2. Use said `SimpleImputer` to impute the missing values\n",
    "3. Ordinal encode all the categorical features.\n",
    "\n",
    "All in one, single, amazing, Megazord.\n",
    "\n",
    "## 3.4 Custom transformers\n",
    "\n",
    "We can build own custom transformers, for as long as they follow the usual blueprint:\n",
    "* Implement `Transformer.fit()`\n",
    "* And `Transformer.transform()`.\n",
    "\n",
    "All scikit-learn estimators have `get_params()` and `set_params` functions. \n",
    "\n",
    "The easiest way to implement these functions sensibly is to inherit from `sklearn.base.BaseEstimator`, as we're doing.\n",
    "\n",
    "And `Pipeline` compatibility requires a `fit_transform()` method that we are inheriting from `sklearn.base.TransformerMixin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMultiplier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, some_parameter):\n",
    "        self.some_parameter = some_parameter\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the transformer and store it.\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        # Transform X.\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we may want our transformer to accept some parameters.\n",
    "\n",
    "That's we are doing when we include `some_parameter` in the `__init__`.\n",
    "\n",
    "Back to our transformers. Our blueprint:\n",
    "* We want the estimator to be able to take a `strategy` parameter, although we will support only the mode\n",
    "* Fitting requires taking the mode of each column and storing it\n",
    "* Transform implies replacing missing values with the given column modes.\n",
    "\n",
    "How are we going to compute the modes? Pandas, as always, provides a convenient `.mode()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use indexing we will use `df.squeeze()`, a convenient method to transform our dataframe into a `pd.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.mode().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy=None):\n",
    "        if strategy:\n",
    "            self.strategy = strategy\n",
    "        else:\n",
    "            self.strategy = 'most_frequent'\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.strategy == 'most_frequent':\n",
    "            self.fills = X.mode(axis=0).squeeze()\n",
    "            return self\n",
    "        else:\n",
    "            return 'Strategy not supported.'\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X).fillna(self.fills)\n",
    "\n",
    "\n",
    "imputer = CategoryImputer()\n",
    "X_train_ = imputer.fit_transform(X_train_)\n",
    "X_train_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! What about the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = imputer.transform(X_test_)\n",
    "X_test_.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Victory awaits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Everything together\n",
    "\n",
    "Now, we want to fill in missing values and use one-hot-encoding (remember?), all at the same time.\n",
    "\n",
    "We are reaching our destination!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megazord = Pipeline([('fill_na', CategoryImputer(strategy='most_frequent')),\n",
    "                     ('encode', OrdinalEncoder())])\n",
    "\n",
    "X_train_ = megazord.fit_transform(X_train)\n",
    "X_test_ = megazord.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we abstract all the logic of passing transformers around.\n",
    "\n",
    "Now, can we throw a model in there? Perhaps we can.\n",
    "\n",
    "(But we shouldn't, in a way. Please note that we are exemplifying data wrangling workflows.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megazord = Pipeline([('fill_na', CategoryImputer(strategy='most_frequent')),\n",
    "                     ('encode', OrdinalEncoder()),\n",
    "                     ('k_means', KMeans())])\n",
    "\n",
    "megazord.fit(X_train_)\n",
    "megazord.predict(X_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we are encoding categorical variables as if they were ordinal, instead of using one-hot-encoding, as recommended.\n",
    "\n",
    "Take this for what it is: an example on how to build end-to-end pipelines for modeling in scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
