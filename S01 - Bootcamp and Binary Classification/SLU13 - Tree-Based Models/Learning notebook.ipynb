{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based models - Learning notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from IPython.display import Image\n",
    "\n",
    "from utils.utils import (\n",
    "    make_data,\n",
    "    separate_target_variable,\n",
    "    process_categorical_features,\n",
    "    visualize_tree\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Decision trees](#decision-trees)\n",
    "2. [Tree-based ensembles](#tree-based-ensembles)\n",
    "3. [Bagging with random forests](#bagging-with-random-forests)\n",
    "4. [Boosting with gradient boosting](#boosting-with-gradient-boosting)\n",
    "\n",
    "# Decision trees <a class=\"anchor\" id=\"decision-trees\"></a>\n",
    "\n",
    "Knowledge-based systems represent knowledge explicitly, as a set of rules, rather than being implicit in algorithms. \n",
    "\n",
    "Imagine we want to decide whether to go hiking based on the weather. Our acquired knowledge can be represented as:\n",
    "\n",
    "<img src=\"assets/simple-decision-tree.png\" alt=\"simple-decision-tree\" style=\"width: 600px;\"/>\n",
    "\n",
    "*Fig. 1: A simple decision-tree used for classification (Quinlan, 1986).*\n",
    "\n",
    "This particular structure, a **decision tree**, displays a flow of conditional statements (i.e., `if Condition then Outcome`) where:\n",
    "\n",
    "* Each **node** represents a test on a feature\n",
    "* Each **branch** represents the outcome of the test\n",
    "* Each **leaf** represents an outcome or decision.\n",
    "\n",
    "The paths from root to leaf represent the rules, e.g., `if Outlook is Sunny and Humidity is Normal then Go Hiking`.\n",
    "\n",
    "Pioneering knowledge-based systems codified rules coming from domain specialists, at the rate of a few rules per man day.\n",
    "\n",
    "But complex tasks require creating and maintaining thousands of such rules, turning this specialist-based approach into a bottleneck.\n",
    "\n",
    "## Learning sets of rules as decision trees\n",
    "\n",
    "The need to discover and maintain sets of rules to make decisions stimulated the investigation of general-purpose machine learning methods to build knowledge-based systems.\n",
    "\n",
    "The resulting systems address the underlying task of learning simple rules, by building decision trees from data examples.\n",
    "\n",
    "Typically, a decision tree is developed from the top-down (known as top-down induction), and there are several algorithms that can be used to build them, including:\n",
    "* **Iterative Dichotomiser 3 (ID3)**, for classification using categorical features\n",
    "* **C4.5**, extends ID3 to support non-categorical features\n",
    "* **Classification and Regression Trees (CART)**, generalizes C4.5 to support regression.\n",
    "\n",
    "We follow ID3 ([Quinlan, 1986][1]), for binary classification using categorical features with a small set of possible values (i.e. low cardinality).\n",
    "\n",
    "[1]: http://hunch.net/~coms-4771/quinlan.pdf \"Quinlan, J. 1986. Induction of Decision Trees. Machine Learning 1: 81-106.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm overview\n",
    "\n",
    "Take an arbitrary training set $C = \\{\\textbf{x}_i, y_i\\}_{i=1}^m$, a collection of labeled observations, with $y_i \\in \\{L_1, \\dots, L_v\\}$.\n",
    "\n",
    "The features vector is represented as $\\textbf{x}_i = \\{x_i^{(1)}, \\dots, x_i^{(n)}\\}$, where $x_i^{(j)}$ is the value of the $j$-th feature for the $i$-th observation.\n",
    "\n",
    "In ID3, $x_i^{(j)}$ can take on one of a fixed number of possible values $x_i^{(j)} \\in \\{A_1^{(j)}, \\dots, A_w^{(j)}\\}$.\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "  x_1^{(1)} & x_1^{(2)} & \\dots & x_1^{(n)} \\\\\n",
    "  x_2^{(1)} & x_2^{(2)} & \\dots & x_2^{(n)} \\\\\n",
    "  \\dots     & \\dots     & \\dots & \\dots     \\\\\n",
    "  x_m^{(1)} & x_m^{(2)} & \\dots & x_m^{(n)}\n",
    " \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = separate_target_variable(data)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a binary classification problem, where $y_i \\in \\{0, 1\\}.$\n",
    "\n",
    "$$y = \\begin{bmatrix}\n",
    "  y_1   \\\\\n",
    "  y_2   \\\\\n",
    "  \\dots \\\\\n",
    "  y_3 \n",
    " \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine a test, $T$, on $x_i$, with possible outcomes $O_1, O_2, \\dots, O_w$. \n",
    "\n",
    "It can be a condition on the features, for example.\n",
    "\n",
    "$T$ produces a partition $\\{C_1, C_2, \\dots, C_w\\}$ of $C$, where $C_k$ contains those observations having outcome $O_k$.\n",
    "\n",
    "<img src=\"assets/test-partitioning.png\" alt=\"test_partitioning\" style=\"width: 600px;\"/>\n",
    "\n",
    "*Fig. 2: Partitioning of the objects in $C$ with a test $T$ (Quinlan, 1986).*\n",
    "\n",
    "We can recursively apply different tests to each of these partitions, creating smaller and smaller sub-partitions, until we get homogenous (i.e., single-class) leaves. The result of this process is a decision tree for all of $C$.\n",
    "\n",
    "This tree-building algorithm can be described in the following way:\n",
    "```\n",
    "ID3 (Data, Target, Attributes)\n",
    "    If all examples are positive, Return the single-node tree Root, with label = 1.\n",
    "    If all examples are negative, Return the single-node tree Root, with label = 0.\n",
    "    Otherwise Begin\n",
    "        A <- Pick the Attribute that best classifies examples.\n",
    "        Decision tree for Root = A.\n",
    "        For each possible value, O_i, of A,\n",
    "            Add a new tree branch, corresponding to the test A = O_i.\n",
    "            Let Data(v_i) be the subset of examples that have the value O_i for A.\n",
    "            Below this new branch add the subtree ID3 (Data(O_i), Target, Attributes â€“ {A}).\n",
    "    End\n",
    "    Return Root\n",
    "```\n",
    "\n",
    "\n",
    "But what is the **attribute that best classifies examples** at each step? As we will soon see, this is related with the concepts of **information gain** and **entropy**.\n",
    "\n",
    "## Attribute selection\n",
    "\n",
    "A test will have a high **information gain** if it generates partitions with low [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)). In general: the greater the homogeneity of a partition, the lower its entropy.\n",
    "\n",
    "In this case, a partition is said to be very homogeneous if the majority of its elements belong to a single class. \n",
    "\n",
    "Consider the problem of binary classification (it can be extended to support multi-class settings). Take $p$ as the probability of the positive class, i.e., the proportion of positive cases in the set. In this case, the entropy is given by:\n",
    "\n",
    "$$I(p) = - p \\log_2 p - (1 - p) \\log_2 (1-p)$$\n",
    "\n",
    "\n",
    "It ranges between 0 and 1:\n",
    "\n",
    "* 0, when $p=1$ or $p=0$. We are in a situation of low entropy/high purity, because there is no \"surprise\" in the outcome. We say the information is minimum, because observing the class of an instance will not give us any new information - we already know what it will be. \n",
    "* 1, when $p=\\frac{1}{2}$. We are in a situation of high entropy/low purity, because the \"uncertainty\" of the outcome is maximum. This is equivalent to saying that the information is maximum.\n",
    "\n",
    "Out of curiosity, entropy is measured in **bits** (hence the $log_2$ in the formula, since it tells you the number of digits you would need to represent a number in base 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    if 0 < p < 1:\n",
    "        return -p * log2(p) - (1 - p) * log2(1 - p)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the entropy for the entire data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probability(data):\n",
    "    n = data.shape[0]\n",
    "    f = (data['Class'] == 1).sum()\n",
    "\n",
    "    return f / n\n",
    "\n",
    "\n",
    "p = compute_probability(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a fairly high entropy for the entire data set.\n",
    "\n",
    "This means that substantial proportions of the data set belong to each of the classes, as confirmed by the value of $p$ - the data is not homogenous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from a set with high entropy, we would like to select a test that divides it into partitions that are as homogeneous as possible. In the ideal case, each partition would be a leaf node, containing only instances from a single class.\n",
    "\n",
    "A test is restricted to branching on an attribute $A$ with values $\\{A_1, A_2, \\dots, A_v\\}$, thus partitioning $C$ into $\\{C_1, C_2, \\dots, C_v\\}$. \n",
    "\n",
    "The expected entropy of the test is obtained as a weighted average of the entropy of the resulting groupings:\n",
    "\n",
    "$$E(A) = \\sum_{i=1}^v \\frac{\\|C_i\\|}{\\|C\\|}I(p_i)$$\n",
    "\n",
    "In short, and as expected, we are measuring an attribute's ability to generate homogeneous groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the expected information of the `Outlook` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_entropy(data, attribute):\n",
    "    c_norm = data.shape[0]\n",
    "    information = 0\n",
    "    values = data[attribute].unique()\n",
    "    for value in values:\n",
    "        group = data[data[attribute] == value]\n",
    "        \n",
    "        c_i_norm = group.shape[0]\n",
    "        \n",
    "        w = c_i_norm / c_norm\n",
    "        \n",
    "        p = compute_probability(group)\n",
    "        e = entropy(p)\n",
    "    \n",
    "        information += w * e\n",
    "    \n",
    "    return information\n",
    "\n",
    "\n",
    "mean_entropy(data, 'Outlook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this result, the `Outlook` feature yields partitions that have, on average, a much lower entropy than the entire data set. It is a good candidate for the first test!\n",
    "\n",
    "We call this loss of entropy the **information gain** of branching on an attribute $A$, and it is given by:\n",
    "\n",
    "$$IG(A) = I(p) - E(A)$$\n",
    "\n",
    "where $I(p)$ is the entropy on the node over which the test is being performed, and $E(A)$ is the expected entropy of the test.\n",
    "\n",
    "Let's compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(data, attribute):\n",
    "    p = compute_probability(data)\n",
    "    i = entropy(p)\n",
    "    \n",
    "    e = mean_entropy(data, attribute)\n",
    "    \n",
    "    return i - e\n",
    "\n",
    "\n",
    "information_gain(data, 'Outlook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the simplest possible tree, it is a good choice to branch, at each step, on the attribute with the highest information gain. Let's find out which:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_candidate_attributes(data, attributes):\n",
    "    return {\n",
    "        attribute:information_gain(data, attribute) \n",
    "        for attribute \n",
    "        in attributes\n",
    "    }\n",
    "\n",
    "\n",
    "attributes = [c for c in data.columns if c is not 'Class']\n",
    "examine_candidate_attributes(data, attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we would start by branching on the values of the `Outlook` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlook_values = data['Outlook'].unique()\n",
    "outlook_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the process in each partition generated by the `Outlook` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, value in enumerate(outlook_values):\n",
    "    partition = data[data['Outlook'] == value]\n",
    "    results = examine_candidate_attributes(partition, attributes)\n",
    "    print(\"\\n---\\n\")\n",
    "    print(\"Partition: Outlook == {}. \\nResults:\\n{}.\".format(value, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, none of the attributes provides any information gain in the `Outlook == overcast`  case. Let's try to understand why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Outlook'] == 'overcast']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, this grouping is already homogeneous (all instances are positive) - it is already a leaf. We figured out our first rule:\n",
    "\n",
    "```\n",
    "if Outlook is Overcast then Go Hiking\n",
    "```\n",
    "\n",
    "Finally, we would continue partitioning recursively until all the resulting groupings are homogeneous (we obtain pure leaves), or until we have exhausted our features (in which case, we assign to the final leaves the majority class in their partition).\n",
    "\n",
    "## Using decision trees\n",
    "\n",
    "The `sklearn` implementation uses an optimized version of the CART algorithm. This means that the resulting decision tree might be different than if you had used ID3. \n",
    "\n",
    "We won't go into this in detail, but don't worry - the principles behind it are very similar to what you've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = process_categorical_features(X)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=101)\n",
    "clf.fit(X_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we do a trick to convert categorical variables into something more `sklearn`-friendly.\n",
    "\n",
    "(ignore it for now - you will learn all about it in due time.)\n",
    "\n",
    "Then, we train a `DecisionTreeClassifier` (refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for more information).\n",
    "\n",
    "Let's visualize the resulting tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Don't go hiking!\", \"Go hiking!\"]\n",
    "t = visualize_tree(clf, X_.columns, class_names)\n",
    "Image(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each node, we can see:\n",
    "\n",
    "* two different paths, indicated by each result of the test `(feature_value <= 0.5)`, applied to the current node. `True` means **all instances in this node for which `feature` IS NOT equal to `value`**. The left path is always True, and the right path is always False.\n",
    "* the entropy in that node.\n",
    "* the `value` array, which indicates how many samples reaching that node belong to each class (the classes are in ascending numerical order)\n",
    "* the name of the majority class in the node (in case of a tie, the first class in numerical order is taken)\n",
    "\n",
    "The CART algorithm can also be used for regression problems. In that case, you should use the `DecisionTreeRegressor` (refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)). To visualize a simple example, check out [this page](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "A great characteristic of decision trees is that they allow us to compute **feature importance**: a score for each feature, based on how useful they are at predicting the target variable.\n",
    "\n",
    "The importance of a feature is given by the decrease in node impurity (according to the criterion being used - in this case, the entropy), weighted by the proportion of samples reaching the node.\n",
    "\n",
    "We can calculate the feature importance in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(data=clf.feature_importances_, index=X_.columns)\n",
    "feature_importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the visual representation of the tree, we can see that the `Windy_true` attribute has the highest information gain (from 1 to 0 on both leaves it generate), and therefore the highest decrease in node impurity. \n",
    "\n",
    "However, since not many samples reach that node (only 2 in the training set), this is not the most important feature, since it is not as relevant to predicting the class in the overall dataset. Compare it with the feature with the highest importance, `Outlook_overcast`, placed right at the root of the tree: despite not having as high of an information gain, it immediately allows us to classify 4 samples as \"Go Hiking\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and cons\n",
    "\n",
    "### Pros\n",
    "\n",
    "Decision trees are a straightforward way to represent rules that:\n",
    "\n",
    "* are simple to understand, interpret and visualize\n",
    "* require little to no data preparation (for example, don't require data scaling and normalization)\n",
    "* are able to handle numerical and categorical variables\n",
    "* are a white-box model, all decisions are replicable and easily explainable.\n",
    "\n",
    "### Cons\n",
    "\n",
    "Decision trees are extremely flexible and prone to creating over-complex trees that don't generalize:\n",
    "\n",
    "* Overfitting\n",
    "* Overfitting\n",
    "* Overfitting.\n",
    "\n",
    "(Repetition makes perfect.)\n",
    "\n",
    "Mechanisms such as pruning (removing sections of the tree) and setting the maximum depth of the tree help with overfitting. Let's see how we can configure the maximum tree depth with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    max_depth=2, \n",
    "    random_state=101\n",
    ")\n",
    "clf.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = visualize_tree(clf, X_.columns, class_names)\n",
    "Image(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the minimum number of samples required to split a node, in order to avoid fully-grown trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    min_samples_split=5, \n",
    "    random_state=101\n",
    ")\n",
    "clf.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = visualize_tree(clf, X_.columns, class_names)\n",
    "Image(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all help with controlling overfitting, but they might simply not be enough; and in fact, some times this approach is too heavy-handed, and might lead to underfitting. We will need to find a smarter way to control overfitting, while still allowing our trees to represent complex rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some final notes:\n",
    "\n",
    "* if the attributes are adequate, it is always possible to construct a decision-tree that correctly classifies every training instance.\n",
    "* attributes are **inadequate** if the data contains two objects that have identical values for each attribute and yet belong to different classes.\n",
    "* the key takeaway: **decision trees overfit like hell**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based ensembles <a class=\"anchor\" id=\"tree-based-ensembles\"></a>\n",
    "\n",
    "**Ensemble methods** combine the predictions of several models, known as **base learners** or **base estimators**. Each individual estimator in the ensemble is often a \"weak learner\" (i.e. has slightly better accuracy than random). However, combining the predictions of the entire ensemble, we often get better results that are less prone to overfitting, when compared to using a single, larger model.\n",
    "\n",
    "\n",
    "<img src=\"assets/ensemble-methods.png\" alt=\"ensemble-methods\" style=\"width: 600px;\"/>\n",
    "\n",
    "*Fig. 3: A simple ensemble model, using many trained models to generate a single prediction.*\n",
    "\n",
    "There are homogeneous and heterogenous ensembles, based on whether the base learners are all of the same type or not.\n",
    "\n",
    "We focus on homogenous ensembles of decision-trees, particularly:\n",
    "\n",
    "* Building several independent trees and then average their predictions, so that variance is reduced (i.e., **bagging**)\n",
    "* Building trees sequentially as to reduce the bias of the combined estimator (i.e., **boosting**).\n",
    "\n",
    "# Bagging with random forests <a class=\"anchor\" id=\"bagging-with-random-forests\"></a>\n",
    "\n",
    "Bootstrap aggregating, also known as bagging, consists in:\n",
    "\n",
    "1. Creating several independent data sets \n",
    "2. Training a model in each data set\n",
    "3. Aggregating individual predictions.\n",
    "\n",
    "Bagging can be seen as training several independent models in parallel and averaging the predictions.\n",
    "\n",
    "Imagine the following example: you are experiencing a strange feeling in your nose, and want to get a diagnosis. You have the option of going to a single, extremely-specialized podiatrist (that's a foot doctor, in case you didn't know, and in this analogy, it represents a single large model), or you could get your diagnosis from an association of 500 3rd year university students of medicine (which in this analogy represent a bag of simpler, shallower models).\n",
    "\n",
    "Now, the podiatrist is very specialized: one might even say that he might have **overfit** to foot diseases. He might be able to extend his knowledge to your particular affliction and give a good diagnosis, but odds are he will diagnose you with something totally unrelated.\n",
    "The group of 3rd year medicine students, on the other hand, might lack individual expertise; but if each of them has at least attended *some* classes, and *if they haven't all attended the same classes*, it's very likely that their collective knowledge surpasses the podiatrist's, and that their majority vote will be the right diagnosis. \n",
    "\n",
    "This example illustrates the main strength of bagging, which is that several weak models can cancel each other's weaknesses, provided that they are highly independent and were exposed to different information. \n",
    "\n",
    "But how does this relate to decision trees, you might ask? Well, decision-trees don't simply overfit, they are also highly unstable: small variations in the data result in wildly different trees.\n",
    "\n",
    "This makes them particularly suited for bagging.\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Suppose you have a sequence of data sets $\\{D_1, D_2, \\dots, D_k\\}$, with observations from the same underlying distribution $\\mathcal{D}$.\n",
    "\n",
    "We can obtain an **ensemble of models**, $\\{h_1, h_2, \\dots, h_k\\}$, by training a model in each data set.\n",
    "\n",
    "<img src=\"assets/bagging-estimator.png\" alt=\"bagging-estimator\" style=\"width: 600px;\"/>\n",
    "\n",
    "*Fig. 4: In bagging different data sets are generated from the main one, models are trained in parallel and predictions averaged.*\n",
    "\n",
    "By running the models in parallel, we get $\\{\\hat{y}_1\\, \\hat{y}_2\\, \\dots, \\hat{y}_k\\}$, a list of predictions from our ensemble.\n",
    "\n",
    "To obtain a single prediction we can then evaluate all models and aggregate the results by:\n",
    "* Averaging the $k$ predictions (regression)\n",
    "* Using majority voting to predict a class (classification).\n",
    "\n",
    "More often than not, however, we don't have multiple data sets. In this situation, we can do bootstrapping.\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "We can take repeated random samples with replacement $\\{C^1, C^2, \\dots, C^b\\}$ from $C$, our full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bootstrap_data(data, b):\n",
    "    n = data.shape[0]\n",
    "    return [data.sample(n=n, replace=True) for i in range(b)]\n",
    "\n",
    "\n",
    "bootstrap_data = make_bootstrap_data(data, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the bootstrapped data sets are the same size as the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now train a different decision tree in each data set and use voting to predict whether or not to go hiking.\n",
    "\n",
    "Bagging works very well in controlling overfitting and the generalization error in unstable models. If one of the models in the bag overfits to a certain noisy observation, it's very likely that its vote will be drowned out by the rest of the models in the ensemble.\n",
    "\n",
    "## Random forests\n",
    "\n",
    "A **random forest** is an ensemble learning method created by bagging multiple decision trees.\n",
    "In random forests, bagging is used in tandem with random feature selection:\n",
    "\n",
    "* Datasets are generated from the original data, using bootstrapping (row sampling)\n",
    "* Then a tree is grown on each bootstrapped data set using random feature selection (column sampling).\n",
    "\n",
    "Random feature selection means that only a random subset of the features is available at each split.\n",
    "\n",
    "Randomizing features acts as a kind of regularization, further mitigating overfitting, because it forces each individual classifier to be as good as possible, while having access to limited information. This increases diversity inside the ensemble, which is often very beneficial.\n",
    "\n",
    "Fortunately, `sklearn` implements all this intricate logic for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=10, \n",
    "    criterion='entropy',\n",
    "    max_features=2, \n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "rf.fit(X_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed information about the models, check the documentation:\n",
    "* RandomForestClassifier ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))\n",
    "* RandomForstsRegressor ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)).\n",
    "\n",
    "Bagging is a great technique to reduce overfitting, and as such, to reduce the variance of our final predictions. This comes at the cost of interpretability - it is much easier to interpret the rules contained within a single decision tree, than it is to analyze the driving forces in a majority vote of hundreds or even thousands of models. \n",
    "\n",
    "# Boosting with gradient boosting <a class=\"anchor\" id=\"boosting-with-gradient-boosting\"></a>\n",
    "\n",
    "**Boosting** is a different ensemble learning technique. In this case, instead of training several base learners in parallel and averaging their predictions, we train them sequentially; and the input of each model is the residual error of the previous model.\n",
    "\n",
    "The general idea is to build strong ensembles by combining base learners sequentially, each of them correcting previous errors (and thus reducing bias).\n",
    "\n",
    "**Note:** this section can be pretty intimidating at first, but don't despair! It will be worth it in the end.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Trees are grown sequentially, and each tree is created using information derived from the previous tree. In each iteration:\n",
    "\n",
    "1. Errors and misclassifications of the past model are given increased weight in a new training data set\n",
    "2. The current model is trained on the new training set, fitting the residual errors of the previous model.\n",
    "\n",
    "As such, each model specializes in correcting past mistakes and misclassifications. \n",
    "\n",
    "<img src=\"assets/boosting-estimator.png\" alt=\"boosting-estimator\" style=\"width: 600px;\"/>\n",
    "\n",
    "Intuitively, this produces an ensemble of models that are good in different \"parts\" of the training data, as they sequentially correct each other's mistakes. The final prediction is obtained by summing the predictions of each model in the ensemble.\n",
    "\n",
    "Boosting is less robust than bagging against overfitting, and as such, it is recommended to control the number of estimators used, the strength of each individual estimator, the learning rate, and use other regularization techniques.\n",
    "\n",
    "\n",
    "In this example we will use the [boston](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) house-prices data set (regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_boston():\n",
    "    boston = load_boston()\n",
    "    X = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "    y = pd.Series(data=boston.target, name='price')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_boston, y_boston = prepare_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we mean by \"gradient\" boosting\n",
    "\n",
    "To understand the meaning of **gradient boosting**, we need to review a couple of concepts.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "A loss function quantifies how bad our predictions are. An example is the squared error, seen below: \n",
    "\n",
    "$$L_i = (y_i - h(x_i))^2$$ \n",
    "\n",
    "As we can see, the square error increases with the square of the difference between each of our predictions, and the real value. The worse this error across all data samples, the worse will the overall model performance be.\n",
    "\n",
    "The training of an individual machine learning model is driven by the minimization the total loss, over all observations in the training set, with respect to the model's parameters $\\theta$: \n",
    "\n",
    "$$\\min_{\\theta} J = \\sum_{i=1}^N L(y_i, h(x_i, \\theta))$$\n",
    "\n",
    "$h(x_i, \\theta)$ represents our model's predictions, or $\\hat{y}$.\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "The gradient of a loss function ($\\nabla$), at each point, is its inclination or slope.\n",
    "\n",
    "It's the fancy term for the multi-variable generalization of the derivative, i.e., the partial derivatives of function $f$ at a given point.\n",
    "\n",
    "When training a model by gradient descent, we calculate, at each training step, the derivative of the loss $L$ with respect to the model's parameters, and then update the parameters by giving a small step in the opposite direction of this gradient - the direction in which the loss is decreasing:\n",
    "\n",
    "$$\\theta^{n+1} = \\theta^{n} - \\eta \\nabla_{\\theta} L(y, \\hat{y})$$\n",
    "\n",
    "As such, at each step, we are trying to gradually move our model to a location in parameter space where the loss function has a lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting in detail\n",
    "\n",
    "We now have all pieces required to understand gradient boosting. Let's consider the case of a numerical target variable.\n",
    "\n",
    "We have our first model in the boosting ensemble, $F^{(1)}(x, \\theta)$. This model achieves a certain loss, given by $L(\\textbf{y}, F^{(1)}(x, \\theta))$. \n",
    "\n",
    "We now want to add a second model, $h$, to improve our first model in a way that \n",
    "\n",
    "$$\n",
    "F^{(1)}(x_1) + h(x_1) = y_1 \\\\\n",
    "F^{(1)}(x_2) + h(x_2) = y_2 \\\\\n",
    "...\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "h(x_1) = y_1 - F^{(1)}(x_1) \\\\\n",
    "h(x_2) = y_2 - F^{(1)}(x_2) \\\\\n",
    "...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe there isn't a single model that can achieve this. But perhaps some regression tree can do it approximately. We can try to fit it to the following data points:\n",
    "\n",
    "$$\n",
    "(x_1, y_1 - F^{(1)}(x_1)) \\\\\n",
    "(x_2, y_2 - F^{(1)}(x_2)) \\\\\n",
    "...\n",
    "$$\n",
    "\n",
    "\n",
    "$y_i - F^{(1)}(x_i)$ are called **residuals**. The role of $h$ is to be strong where the initial model was weak. And if the new model $F^{(2)} = F^{(1)} + h$ still isn't good enough, we can fit an additional regression tree to its residuals, and so on and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a step back to our first model, imagine we are using the squared error loss function:\n",
    "\n",
    "$$\n",
    "L(\\textbf{y},F_1(x)) = (\\textbf{y} - F^{(1)}(x))^2\n",
    "$$\n",
    "\n",
    "We want to minimize $J = \\sum_i{L(y_i, F^{(1)}(x_i))}$ by adjusting $F^{(1)}(x_1)$, $F^{(1)}(x_2)$, ...\n",
    "\n",
    "Now, what we are truly adjusting during learning are the parameters $\\theta$ of $F^{(1)}$. But if we take a functional perspective, we can observe that $F^{(1)}(x_i)$ are simply numbers - we can treat them as parameters and take derivatives:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial F^{(1)}(x_i)} & = \\frac{\\partial \\sum_{j=0}^{n}{L(y_j, F^{(1)}(x_j))}}{\\partial F^{(1)}(x_i)} \\\\\n",
    "& = \\frac{\\partial L(y_i, F^{(1)}(x_i))}{\\partial F^{(1)}(x_i)} \\\\\n",
    "& = -2 (y_i - F^{(1)}(x_i)) \\\\\n",
    "& = -2*residuals\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, when the loss function is the squared error, the residuals are proportional to the negative gradients of the loss function!\n",
    "\n",
    "Thus arises the similarity with gradient descent:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F^{(2)}(x_i) & = F^{(1)}(x_i) + h(x_i) \\\\\n",
    "& = F^{(1)}(x_i) + y_i - F^{(1)}(x_i) \\\\\n",
    "& = F^{(1)}(x_i) - \\frac{1}{2}\\frac{\\partial J}{\\partial F^{(1)}(x_i)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^{n} - \\eta \\frac{\\partial J}{\\partial F(\\theta_i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize: for regression with **square loss**,\n",
    "\n",
    "* the residual <=> negative gradient\n",
    "* fitting h to the residual <=> fitting h to the negative gradient\n",
    "* adding a new estimator based on the residual <=> adding a new estimator based on the negative gradient\n",
    "\n",
    "When adding a new estimator to our boosting ensemble, we are actually minimizing a global loss function by using gradient descent, in function space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing to understand here is that the concept of gradients is more useful than the concept of residuals, because it allows us to generalize to loss functions other than the squared error. As long as we fit each additional model to the negative gradients of the current global model, we will be minimizing our desired global loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In practice\n",
    "\n",
    "Let's implement one step of a boosting algorithm, for the case where the loss function is the mean squared error, based on what we have discussed so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "We initialize all $F^{(1)}(x_i)$ to a sensible constant, $F^{(1)}(x_i) = \\gamma$.\n",
    "\n",
    "Since our goal is to minimize the mean squared error, we will use the mean value of the target variable in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_boston_h0_pred = np.repeat(y_boston.mean(), y_boston.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the mean squared error of this initial set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_boston, y_boston_h0_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations\n",
    "\n",
    "#### Generating a new data set\n",
    "\n",
    "We build a new training set at the start of each iteration, by recomputing the target variable to be the gradient of the previous model.\n",
    "\n",
    "More concretely, at the $j$-th iteration we compute a new target variable $r^{(j)}_i$, corresponding to the gradient of the past iteration:\n",
    "\n",
    "$$r^{(j)}_{i} = \\frac{\\partial L(y_i, F^{(j-1)}(x_i))}{\\partial F^{(j-1)}(x_i)} = -2(r^{(j-1)}_{i} - F^{(j-1)}(x_i))$$\n",
    "\n",
    "For the first iteration:\n",
    "\n",
    "* $r^{(j-1)}_i = y_i$, the original targets\n",
    "* $F^{(j-1)}(x_i)) = F^{(1)}(x_i) = \\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, y_pred):\n",
    "    return -2 * (y - y_pred)\n",
    "\n",
    "\n",
    "y_boston_h0_residual = compute_gradient(y_boston, y_boston_h0_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a decision-tree\n",
    "\n",
    "We now fit a new decision-tree to the negative gradient that we just calculated, $r^{(j)}_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(max_depth=1)\n",
    "dt.fit(X_boston, y_boston_h0_residual)\n",
    "\n",
    "y_boston_h0_residual_pred = dt.predict(X_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we will want to train the simplest, shallowest decision-trees as base learners, to avoid overfitting (which is why we are setting the maximum depth of the tree to one.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the global model\n",
    "\n",
    "We now want to add the predictions made by this model (over the residuals) to the predictions made by the base model. The update rule goes as follows, where $\\eta$ is a **learning rate** that controls the magnitude of the update and acts as a regularizer:\n",
    "\n",
    "$$F^{(j)}(x_i)) = F^{(j-1)}(x_i)) - \\eta \\cdot \\frac{\\partial L(y_i, F^{(j-1)}(x_i))}{\\partial F^{(j-1)}(x_i)}$$\n",
    "\n",
    "The learning rate is also known as the *shrinkage factor*, as it shrinks the impact of the corrections if $\\eta$ between 0 and 1.\n",
    "\n",
    "Since we don't know the true value of $y_i$ at prediction time, we plug-in the prediction of the decision-tree we  fit in the previous step:\n",
    "\n",
    "$$F^{(j)}(x_i)) \\approx F^{(j-1)}(x_i)) - \\eta \\cdot \\hat{r}^{(j)}_i(x_i) $$\n",
    "\n",
    "If everything goes well, with each step we are moving closer and closer to the function that minimizes the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_predictions(previous_prediction, residual_prediction, lr):\n",
    "    return previous_prediction - lr * residual_prediction\n",
    "\n",
    "\n",
    "y_boston_h1_pred = update_predictions(y_boston_h0_pred, y_boston_h0_residual_pred, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the mean squared error for the updated predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_boston, y_boston_h1_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurrah! The error decreased. We would continue the process until the error stopped decreasing, or until we detected signs of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Suppose we have $m$ individual decision-trees (boosting stages)\n",
    "\n",
    "The final prediction will be given by the sum of the initial constant and all the gradient-based corrections:\n",
    "\n",
    "$$F^m_i(x_i) = \\gamma + \\sum_{j=1}^m \\eta \\cdot \\hat{r}^{(j)}_i(x_i)$$\n",
    "\n",
    "Where $\\hat{r}^{(j)}_i(x_i)$ is the output of a decision-tree that predicts the negative gradient of the previous iteration.\n",
    "\n",
    "A final note: don't worry if you didn't understand all the details of previous section - it was quite heavy on the mathematical side. Just make sure you understand the basics of boosting and the idea behind it, and `sklearn` has got you covered!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With sklearn\n",
    "\n",
    "Enough with theory: `sklearn` to the rescue! Let's create a `GradientBoostingClassifier`, which can be used for classification problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(\n",
    "    learning_rate=.1,\n",
    "    n_estimators=10\n",
    ")\n",
    "\n",
    "gb.fit(X_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include sampling\n",
    "\n",
    "Some implementations provide the ability to sample observations and features at each iteration, similarly to what happens with Random Forests. Unsurprisingly, this reduces overfitting at the expense of increased bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(\n",
    "    learning_rate=.1, \n",
    "    n_estimators=10, \n",
    "    subsample=.5\n",
    ")\n",
    "gb.fit(X_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "You are now armed with the power of bagging and boosting, two techniques you can easily use to obtain models with a lot of expressive power that also generalize better! Hopefully, you also got some insight into the world of more advanced machine learning techniques.\n",
    "\n",
    "Make sure to review this notebook well, and when you're ready, go solve the exercises. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
