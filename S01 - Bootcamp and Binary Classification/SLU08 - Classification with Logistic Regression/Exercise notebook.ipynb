{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bdd5aa2da3067e63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# SLU08 - Classification With Logistic Regression: Exercise notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e2a09fb80383b343",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27488522ce0b142e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this notebook you will practice the following: \n",
    "\n",
    "    - What classification is for\n",
    "    - Logistic regression\n",
    "    - Cost function\n",
    "    - Binary classification\n",
    "    \n",
    "You thought that you would get away without implementing your own little Logistic Regression? Hah!\n",
    "\n",
    "\n",
    "# Exercise 1. Implement the Exponential part of Sigmoid Function\n",
    "\n",
    "\n",
    "In the first exercise you will implement **only the piece** of the sigmoid function where you have to use an exponential. \n",
    "\n",
    "Here's a quick reminder of the formula:\n",
    "\n",
    "$$\\hat{p} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "In this exercise we only want you to complete the exponential part given the values of b0, b1, x1, b2 and x2:\n",
    "\n",
    "$$e^{-z}$$\n",
    "\n",
    "Recall that z has the following formula:\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n",
    "\n",
    "**Hint: Divide your z into pieces by Betas, I've left the placeholders in there!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "exercise1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def exponential_z(b0, b1, x1, b2, x2):\n",
    "    \"\"\" \n",
    "    Implementation of the exponential part of \n",
    "    the sigmoid function manually. In this exercise you \n",
    "    have to compute the e raised to the power -z. Z is calculated\n",
    "    according to the following formula: b0+b1x1+b2x2. \n",
    "    \n",
    "    You can use the inputs given to generate the z.\n",
    "    \n",
    "    Args:\n",
    "        b0 (np.float64): value of the intercept\n",
    "        b1 (np.float64): value of first coefficient \n",
    "        x1 (np.float64): value of first variable\n",
    "        b2 (np.float64): value of second coefficient \n",
    "        x2 (np.float64): value of second variable\n",
    "\n",
    "    Returns:\n",
    "        exp_z (np.float64): the exponential part of\n",
    "        the sigmoid function\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # hint: obtain the exponential part\n",
    "    # using np.exp()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return exp_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "answer1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "value_arr = [1, 1, 2, 2, 0.4]\n",
    "\n",
    "exponential= exponential_z(\n",
    "    value_arr[0], value_arr[1], value_arr[2], value_arr[3], value_arr[4])\n",
    "\n",
    "np.testing.assert_almost_equal(np.round(exponential,3), 0.022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b318c632b90c5f18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Exponential part: 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f40cf4a6bb066df",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Make a Prediction\n",
    "\n",
    "The next step is to implement a function that receives an observation and returns the predicted probability with the sigmoid function.\n",
    "\n",
    "For instance, we can make a prediction given a model with data and coefficients by using the sigmoid:\n",
    "\n",
    "$$\\hat{p} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where Z is the linear equation - you can't use the same function that you used above for the Z part as the input are now two arrays, one with the train data (x1, x2, ..., xn) and another with the coefficients (b0, b1, .., bn).\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "exercise2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(train, coefficients):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns \n",
    "    predicted probabilities for an observation.\n",
    "    \n",
    "    In the train array you will have \n",
    "    the data values (corresponding to the x1, x2, .. , xn).\n",
    "    \n",
    "    In the coefficients array you will have\n",
    "    the coefficients values (corresponding to the b0, b1, .., bn).\n",
    "    \n",
    "    In this exercise you should be able to return a float \n",
    "    with the calculated probabilities given an array of size (1, n). \n",
    "    The resulting value should be a float (the predicted probability)\n",
    "    with a value between 0 and 1. \n",
    "    \n",
    "    Note: Be mindful that the input is completely different from \n",
    "    the function above - you receive two arrays in this functions while \n",
    "    in the function above you received 5 floats - each corresponding\n",
    "    to the x's and b's.\n",
    "    \n",
    "    Args:\n",
    "        train (np.array): a numpy array of shape (n)\n",
    "            - n: number of variables\n",
    "        coefficients (np.array): a numpy array of shape (n + 1, 1)\n",
    "            - coefficients[0]: intercept\n",
    "            - coefficients[1:]: remaining coefficients\n",
    "\n",
    "    Returns:\n",
    "        proba (float): the predicted probability for a data example.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # hint: you have to implement your z in a vectorized \n",
    "    # way aka using vector multiplications - it's different from what you have done above\n",
    "    \n",
    "    # hint: don't forget about adding an intercept to the train data!\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "answer2",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([-1, -1])\n",
    "coefficients = np.array([0 ,3.2, -1])\n",
    "\n",
    "np.testing.assert_almost_equal(round(predict_proba(x, coefficients),3),0.1)\n",
    "\n",
    "x_1 = np.array([-1, -1, 2, 0])\n",
    "coefficients_1 = np.array([0 ,2, -1, 0.2, 0])\n",
    "\n",
    "np.testing.assert_almost_equal(round(predict_proba(x_1, coefficients_1),3),0.354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c2afb353cd17406e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Predicted probabilities for example with 2 variables:  0.0975\n",
    "    \n",
    "    Predicted probabilities for example with 3 variables:  0.3543"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6444271eaf86b4f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 3: Compute the Maximum Log-Likelihood Cost Function\n",
    "\n",
    "As you will implement stochastic gradient descent, you only have to do the following for each prediction, checking how much you will penalize each example according to the difference between the calculated probability and its true value: \n",
    "\n",
    "$$H_{\\hat{p}}(y) =  - (y \\log(\\hat{p}) + (1-y) \\log (1-\\hat{p}))$$\n",
    "\n",
    "In the next exercise you will loop through some examples stored in a array and calculate the cost function for the full dataset. Recall that the formula to generalize the cost function across several examples is: \n",
    "\n",
    "$$H_{\\hat{p}}(y) = - \\frac{1}{N}\\sum_{i=1}^{N} \\left [{ y_i \\ \\log(\\hat{p}_i) + (1-y_i) \\ \\log (1-\\hat{p}_i)} \\right ]$$\n",
    "\n",
    "You will basically simulate what stochastic gradient descent does without updating the coefficients - computing the log for each example, sum each log-loss and then averaging the result across the number of observations in the x dataset/array.\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "exercise3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cost_function(x_values, coef, y):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns the Maximum-Log-Likelihood loss\n",
    "    \n",
    "    Args:\n",
    "        x_values (np.array): array with x training data of size (m, n) shape \n",
    "        where m is the number of observations and n the number of columns\n",
    "        coef (float64): an array with the coefficients to apply of size (1, n+1)\n",
    "        where n is the number of columns plus the intercept.\n",
    "        y (float64): an array with integers with the real outcome per \n",
    "        example.\n",
    "        \n",
    "    Returns:\n",
    "        loss (np.float): a float with the resulting log loss for the \n",
    "        entire data.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # A list of hints that you can follow:\n",
    "    \n",
    "    # you already computed a probability for an example\n",
    "    # so you might be able to reuse the function\n",
    "    \n",
    "    # Store number of examples that you have to loop through\n",
    "    \n",
    "    # Initialize loss\n",
    "    \n",
    "    # if you don't use the function from above to predict probas\n",
    "    # don't forget to add the intercept to the X_array!\n",
    "    \n",
    "    # loop through every example\n",
    "    \n",
    "    # Calculate probability for each example\n",
    "    # Compute log loss\n",
    "    # Hint: maybe separating the log loss will help you\n",
    "    # avoiding get confused inside all the parenthesis\n",
    "    \n",
    "    # Sum the computed loss for the example to the total log loss\n",
    "    \n",
    "    # Divide log loss by the number of examples (don't forget that the log loss\n",
    "    # has to return a positive number!)    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "answer3",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[-1, -1], [3, 0], [3, 2]])\n",
    "coefficients = np.array([[0 ,2, -1]])\n",
    "y = np.array([[1],[1],[0]])\n",
    "np.testing.assert_almost_equal(round(cost_function(x, coefficients, y),3),1.778)\n",
    "\n",
    "x_1 = np.array([[-1, -1], [3, 0], [3, 2], [1, 0]])\n",
    "y_1 = np.array([[1],[1],[0],[1]])\n",
    "\n",
    "np.testing.assert_almost_equal(round(cost_function(x_1, coefficients, y_1),3),1.365)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73aa2b5fc2e95825",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Computed log loss for first training set:  1.77796243\n",
    "    \n",
    "    Computed log loss for second training set:  1.36520382"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb7dca3ebe6d82c8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: Compute a first pass on Stochastic Gradient Descent\n",
    "\n",
    "Now that we know how to calculate probabilities and the cost function, let's do an interesting exercise - computing the derivatives and updating our coefficients. Here you will do a full pass a bunch of examples, computing the gradient descent for each time you see one of them.\n",
    "\n",
    "In this exercise, you should compute a single iteration of the gradient descent! \n",
    "\n",
    "You will basically use stochastic gradient descent but you will have to update the coefficients after\n",
    "you see a new example - so each time your algorithm knows that he saw something way off (for example, \n",
    "returning a low probability for an example with outcome = 1) he will have a way (the gradient) to \n",
    "change the coefficients so that he is able to minimize the cost function.\n",
    "\n",
    "## Quick reminders:\n",
    "\n",
    "Remember our formulas for the gradient:\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_{0(t)}}$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_t}$$\n",
    "\n",
    "which can be simplified to\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p})\\right]$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p}) \\ x \\right]$$\n",
    "\n",
    "You will have to initialize the coefficients in some way. If you have a training set $X$, you can initialize them to zero, this way:\n",
    "```python\n",
    "coefficients = np.zeros(X.shape[1]+1)\n",
    "```\n",
    "\n",
    "where the $+1$ is adding the intercept.\n",
    "\n",
    "Note: We are doing a stochastic gradient descent so don't forget to go observation by observation and updating the coefficients everytime!\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "exercise4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_coefficients(x_train, y_train, learning_rate = 0.1, verbose = False):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns the a first iteration of \n",
    "    stochastic gradient descent.\n",
    "\n",
    "    Args:\n",
    "        x_train (np.array): a numpy array of shape (m, n)\n",
    "            m: number of training observations\n",
    "            n: number of variables\n",
    "        y_train (np.array): a numpy array of shape (m,) with \n",
    "        the real value of the target.\n",
    "        learning_rate (np.float64): a float\n",
    "\n",
    "    Returns:\n",
    "        coefficients (np.array): a numpy array of shape (n+1,)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # A list of hints that might help you:\n",
    "    \n",
    "    # Number of observations\n",
    "    \n",
    "    # initialize the coefficients array with zeros\n",
    "    # hint: use np.zeros()    \n",
    "    \n",
    "    # run the stochastic gradient descent and update the coefficients after \n",
    "    # each observation    \n",
    "    \n",
    "    # compute the predicted probability - you can use a function we have done previously \n",
    "    \n",
    "    # Update intercept\n",
    "    \n",
    "    # Update the rest of the coefficients by looping through each variable\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "answer4",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array([[1,2,3], [2,5,9], [3,1,4], [8,2,9]])\n",
    "y_train = np.array([0,1,0,1])\n",
    "learning_rate = 0.1\n",
    "\n",
    "np.testing.assert_almost_equal(round(compute_coefficients(x_train, y_train, learning_rate)[0],3),-0.001)\n",
    "\n",
    "np.testing.assert_almost_equal(round(compute_coefficients(x_train, y_train, learning_rate)[1],3),0.064)\n",
    "\n",
    "np.testing.assert_almost_equal(round(compute_coefficients(x_train, y_train, learning_rate)[2],3),0.056)\n",
    "\n",
    "np.testing.assert_almost_equal(round(compute_coefficients(x_train, y_train, learning_rate)[3],3),0.139)\n",
    "\n",
    "x_train_1 = np.array([[4,5,2,7], [2,5,7,2], [3,1,2,1], [8,2,9,5], [1,2,9,4]])\n",
    "y_train_1 = np.array([0,1,0,1,1])\n",
    "\n",
    "np.testing.assert_almost_equal(round(compute_coefficients(x_train_1, y_train_1, learning_rate).max(),3) ,0.198)\n",
    "np.testing.assert_almost_equal(round(compute_coefficients(x_train_1, y_train_1, learning_rate).min(),3) ,0.006)\n",
    "np.testing.assert_almost_equal(round(compute_coefficients(x_train_1, y_train_1, learning_rate).mean(),3) ,0.06)\n",
    "np.testing.assert_almost_equal(round(compute_coefficients(x_train_1, y_train_1, learning_rate).var(),3) ,0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a1387ec5d3ac2d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5: Normalize Data\n",
    "\n",
    "To get this concept in your head, let's do a quick and easy function to normalize the data using a MaxMin approach. It is crucial that your variables are adjusted between $[0;1]$ (normalized) or standardized so that you can correctly analyse some logistic regression coefficients for your possible future employer.\n",
    "\n",
    "You only have to implement this formula\n",
    "\n",
    "$$ x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Don't forget that the `axis` argument is critical when obtaining the maximum, minimum and mean values! As you want to obtain the maximum and minimum values of each individual feature, you have to specify `axis=0`. Thus, if you wanted to obtain the maximum values of each feature of data $X$, you would do the following:\n",
    "\n",
    "```python\n",
    "X_max = np.max(X, axis=0)\n",
    "```\n",
    "\n",
    "Not an assertable question but can you remember why it is important to normalize data for Logistic Regression?\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "exercise5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def normalize_data(X):\n",
    "    \"\"\" \n",
    "    Implementation of a function that normalizes your data variables\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): a numpy array of shape (m, n)\n",
    "            m: number of observations\n",
    "            n: number of variables\n",
    "\n",
    "    Returns:\n",
    "        normalized_X (np.array): a numpy array of shape (m, n)\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the numerator first \n",
    "    # you can use np.min()\n",
    "    \n",
    "    # Compute the denominator\n",
    "    # you can use np.max() and np.min()\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return normalized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-334419ee9c78c698",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[7,7,3], [2,2,11], [9,5,2], [0,9,5], [10,1,3], [1,5,2]])\n",
    "normalized_data = normalize_data(data)\n",
    "print('Before normalization:')\n",
    "print(data)\n",
    "print('\\n-------------------\\n')\n",
    "print('After normalization:')\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-194d7aa04c4e007c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Before normalization:\n",
    "    [[ 7  7  3]\n",
    "     [ 2  2 11]\n",
    "     [ 9  5  2]\n",
    "     [ 0  9  5]\n",
    "     [10  1  3]\n",
    "     [ 1  5  2]]\n",
    "\n",
    "    -------------------\n",
    "\n",
    "After normalization:\n",
    "\n",
    "    [[0.7        0.75       0.11111111]\n",
    "     [0.2        0.125      1.        ]\n",
    "     [0.9        0.5        0.        ]\n",
    "     [0.         1.         0.33333333]\n",
    "     [1.         0.         0.11111111]\n",
    "     [0.1        0.5        0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "answer5",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[2,2,11,1], [7,5,1,3], [9,5,2,6]])\n",
    "normalized_data = normalize_data(data)\n",
    "np.testing.assert_almost_equal(round(normalized_data.max(),3),1.0)\n",
    "np.testing.assert_almost_equal(round(normalized_data.mean(),3),0.518)\n",
    "np.testing.assert_almost_equal(round(normalized_data.var(),3),0.205)\n",
    "\n",
    "\n",
    "data = np.array([[1,3,1,3], [9,5,3,1], [2,2,4,6]])\n",
    "normalized_data = normalize_data(data)\n",
    "np.testing.assert_almost_equal(round(normalized_data.mean(),3),0.460)\n",
    "np.testing.assert_almost_equal(round(normalized_data.std(),3),0.427)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Training a Logistic Regression with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will load the Titanic dataset and try to use the available numerical variables to predict the probability of a person surviving the titanic sinking.\n",
    "\n",
    "Prepare to use your sklearn skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load the dataset for you\n",
    "titanic = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you need to do the following: \n",
    "    - Select an array/Series with the target variable (Survived)\n",
    "    - Select an array/dataframe with the X numeric variables (Pclass, Age, Siblings/Spouses Aboard, Parents/Children Aboard and Fare)\n",
    "    - Scale all the X variables - normalize using Max / Min method.\n",
    "    - Fit a logistic regression for maximum of 100 epochs and random state = 100.\n",
    "    - Return an array of the predicted probas and return the coefficients\n",
    "    \n",
    "After this, feel free to explore your predictions! As a bonus why don't your construct a decision boundary using two variables eh? :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "exercise6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_model(dataset):\n",
    "    '''\n",
    "    Returns the predicted probas and coefficients \n",
    "    of a trained logistic regression on the Titanic Dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset(pd.DataFrame): dataset to train on.\n",
    "    \n",
    "    Returns:\n",
    "        probas (np.array): Array of floats with the probability \n",
    "        of surviving for each passenger\n",
    "        coefficients (np.array): Returned coefficients of the \n",
    "        trained logistic regression.\n",
    "    '''\n",
    "    \n",
    "    # leave this np.random seed here\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    \n",
    "    # List of hints:\n",
    "    \n",
    "    # Use the Survived variable as y\n",
    "    # Select the Numerical variables for X \n",
    "    # hint: use pandas .loc or indexing!    \n",
    "    \n",
    "    # Scale the X dataset - you can use a function we have already\n",
    "    # constructed or resort to the sklearn implementation\n",
    "    \n",
    "    # Hint: for epochs look at the max_iter hyper param!\n",
    "    # Fit logistic\n",
    "    \n",
    "    # Obtain probability of surviving\n",
    "    \n",
    "\n",
    "    # Obtain Coefficients from logistic regression\n",
    "    # Hint: see the sklearn logistic regression documentation\n",
    "    # if you do not know how to do this\n",
    "    # No need to return the intercept, just the variable coefficients!\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return probas, coef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "answer6",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "probas, coef = train_model(titanic)\n",
    "\n",
    "# Testing Probas\n",
    "max_probas = probas.max()\n",
    "np.testing.assert_almost_equal(max_probas, 0.892, 2)\n",
    "min_probas = probas.min()\n",
    "np.testing.assert_almost_equal(min_probas, 0.066, 2)\n",
    "mean_probas = probas.mean()\n",
    "np.testing.assert_almost_equal(mean_probas, 0.386, 2)\n",
    "std_probas = probas.std()\n",
    "np.testing.assert_almost_equal(std_probas, 0.183, 2)\n",
    "std_probas = probas.sum()\n",
    "np.testing.assert_almost_equal(std_probas*0.001, 0.340, 2)\n",
    "# Testing Coefs\n",
    "max_coef = coef[0].max()\n",
    "np.testing.assert_almost_equal(max_coef*0.1, 0.11, 1)\n",
    "min_coef = coef[0].min()\n",
    "np.testing.assert_almost_equal(min_coef*0.1, -0.25, 1)\n",
    "mean_coef = coef[0].mean()\n",
    "np.testing.assert_almost_equal(mean_coef*0.1, -0.07, 1)\n",
    "std_coef = coef[0].std()\n",
    "np.testing.assert_almost_equal(std_coef*0.1, 0.15, 1)\n",
    "std_probas = coef[0].sum()\n",
    "np.testing.assert_almost_equal(std_probas*0.1, -0.3, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
