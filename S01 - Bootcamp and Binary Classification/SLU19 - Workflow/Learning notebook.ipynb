{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even if you don't use an import until way later on in the file, put your imports here!\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU19 - Workflow\n",
    "\n",
    "In this notebook we will be covering the following:\n",
    "\n",
    "- Workflow\n",
    "- Workflow Tips\n",
    "- Pipelines and custome estimators\n",
    "\n",
    "The goal for this LU is to establish the common steps and tools that you'll use to keep your data science workflow tight and efficient. If you get a new dataset or are starting out with a hackathon and you find yourself asking yourself \"where and how do I begin?\" this LU is your best friend!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why learn workflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you want to spend your time doing science! Not debugging stupid little things constantly and wasting your time!\n",
    "\n",
    "Also, Data Science is largely an engineering discipline - you might as well just accept that right now. Writing code is an engineering practice and most data science is done with code these days. The most PURE industry research scientist I've spoken to works at Google's Deep Mind lab and he said that he is 50% software engineer! Nailing down a workflow and how to express it in code will make your life an order of magnitude easier. Furthermore, and more importantly, it makes your data science more responsible.\n",
    "\n",
    "You don't want the following:\n",
    "\n",
    "<img src=\"media/xkcd-machine-learning.jpg\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, I need to take a dig at Jupyter:\n",
    "\n",
    "## Jupyter is a terrible development environment\n",
    "\n",
    "Because it isn't one! A development environment is centered around being able to organize your code in an effective way. Jupter is made primarily for rapid prototyping and communication, not software engineering so there are going to be significant drawbacks when it comes to organizing your code and you will need to be extremely anal about following best practices because Jupyter won't do any of it for you the way that a real IDE would.\n",
    "\n",
    "## So why are we using Jupyter?\n",
    "\n",
    "Because our primary task in this academy is not to teach you how to be software engineers. It's to help you learn how to prototype and communicate as data scientists.\n",
    "\n",
    "## No Jupyter in production\n",
    "\n",
    "For the love of god, don't use Juypter notebooks in production. Write code in real .py files that can be tested, properly tracked, diffed, imported into other code, linted in CI/CD, viewed in any editor, and a million other advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get the data\n",
    "\n",
    "In a real live environment, this step could literally take months. It depends on the organization, who guards the data, how well the data itself is known, what format it is in, as well as a million other factors. Throughout the academy we will be largely skipping this step. With the exception of the Data Wrangling Specialization, we will be handing you nice and tidy CSVs that you will be able to bring into your experiment with a single simple function call.\n",
    "\n",
    "I would love to expand a bit more upon the substeps involved in this step but they vary so much in practice that the only thing I can say for certain is that it will involve a lot of meetings and will likely result in reading from a system that behaves a bit like the following:\n",
    "\n",
    "<img src=\"media/xkcd-data-pipeline.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data analysis and preparation\n",
    "\n",
    "This step has some more definitive substeps than the previous. In general you'll hit the following steps:\n",
    "\n",
    "1. Data analysis\n",
    "1. Dealing with data problems\n",
    "1. Feature engineering\n",
    "1. Feature selection\n",
    "\n",
    "### 2.1 Data analysis\n",
    "\n",
    "You've already learned quite a bit about how to do Data Analysis. In SLU01, SLU02, SLU03, SLU04, and SLU05 you have a nice pile of tools that you can use to get a feel for the type of data that you are dealing with. Use them until you feel comfortable enough that you could confidently describe the most important characteristics of the data set you are working with.\n",
    "\n",
    "<img src=\"media/xkcd-quality-data-analysis.png\" width=\"600\"/>\n",
    "\n",
    "### 2.2 Dealing with data problems\n",
    "\n",
    "Your data analysis will certainly uncover data problems. Some of these data problems you may be able to deal with once and others you may need to make part of a pipeline. An example of a data problem that you would deal with once at the beginning of your workflow is changing numbers that are stored as strings in a csv into actual numbers. An example of something that you might want to put off until later is filling in nans so that you could experiment with imputation strategies.\n",
    "\n",
    "In any case, the first time someone delivers you a dataset, the experience is likely to be very much like the following:\n",
    "\n",
    "<img src=\"media/xkcd-dirty-data.png\" width=\"200\"/>\n",
    "\n",
    "### 2.3 Feature engineering\n",
    "\n",
    "Once you've got some clean data and have a benchmark model as a reference you may want to create some new features out of the existing features. A classic example of this would be to create a debt to income ratio feature for credit risk by simply dividing the debt of a person by some measure of their income.\n",
    "\n",
    "You will likely iterate on this step several times.\n",
    "\n",
    "### 2.4 Feature selection\n",
    "\n",
    "You can do feature selection in a few different stages. One stage is right at the beginning when you can remove features that you KNOW for sure should not be in there. Examples of these are features that are all unique, all one value, are leakage, or are disallowed by law. Examples of features that you may want to remove at a later stage are because you found out that they are redundant or don't have any predictive power.\n",
    "\n",
    "You will likely iterate on this step several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train model\n",
    "\n",
    "You know the drill here. Based upon the attributes of the problem at hand (binary classification, multi-class classification, supervised, unsupervised, regression, etc.), choose a few different types of models to experiment with. Note that you should start as simple as possible in order to keep your complexity under control.\n",
    "\n",
    "I'll also take the opportunity to, one more time, stress the importance of creating a training and test set. Never mix the two. Ever.\n",
    "\n",
    "<img src=\"media/not-xkcd-model-training.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate results\n",
    "\n",
    "You've properly separated training and test data, fitted your model, made some prediction on your test sets. Now, depending on the type of problem once again, you need to select a metric or set of metrics to understand how your model is performing. This is also a great time to use learning curves.\n",
    "\n",
    "Try not to suffer from too much tunnel vision here when trying to optimize a single test set on a single metric. That will be tough, especially since the nature of the hackathons in the course are actually all about doing just this... However, when you put a model into production, you won't have the luxury of knowing what your test set will look like so be properly skeptical and be aware of your model's characteristics.\n",
    "\n",
    "Remember, just because something has never happened doesn't mean it may never happen. A model that is overfitted on your training set is blissfully unaware of this. Keep assumptions to a minimum and you'll fail more gracefully when previously unseen things happen.\n",
    "\n",
    "<img src=\"media/xkcd-unseen-data.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a simple baseline FAST\n",
    "\n",
    "We've already mentioned this a few times but it deserves it's own section. Run as quickly as you can toward a simple baseline, no matter how simple it may be. For the specific problem you're working on, it's the data that is important and even the simplest model will give you an idea as to whether or not it has signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incrementally increase complexity\n",
    "\n",
    "Take your super simple baseline model and increase complexity a little bit at a time. Like any responsible scientist, you don't want to be changing more than 1 variable at a time when running experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Scikit pipelines\n",
    "\n",
    "Sooner than later, you will run into the problem of having to do duplicate pre-processing for a training and a test set or for different folds in cross validation. This can be a huge pain the the butt and can result in duplicated code or functions that have a crazy amount of arguments. \n",
    "\n",
    "Let's start of with a bit of motivation by looking at the titanic dataset where we will drop all categorical features and fill the nulls on the rest with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/titanic.csv')\n",
    "X_train, y_train = train_df.drop('Survived', axis=1), train_df.Survived.copy()\n",
    "X_test = pd.read_csv('data/titanic-test.csv')\n",
    "\n",
    "# now let's preprocess and train\n",
    "\n",
    "X_train_clean = X_train.select_dtypes(exclude='object').copy()\n",
    "# note that you will want to impute with the median age from the training set\n",
    "# and NOT the test set. This creates a few difficultites when trying to design\n",
    "# around it\n",
    "X_train_clean['Age'] = X_train_clean.Age.fillna(X_train_clean.Age.median())\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train_clean, y_train)\n",
    "\n",
    "# then to test, we will need to do the same set of preprocessing\n",
    "\n",
    "X_test_clean = X_test.select_dtypes(exclude='object').copy()\n",
    "X_test_clean['Age'] = X_test_clean.Age.fillna(X_train_clean.Age.median())\n",
    "# now it turns out that X_test_clean has a column with nulls that X_test\n",
    "# didn't have so the preprocessing would have to be a bit different\n",
    "\n",
    "# Now there are some nulls in Fare for the test set that were not \n",
    "# in the training set.\n",
    "X_test_clean['Fare'] = X_test_clean.Fare.fillna(X_train.Fare.median())\n",
    "\n",
    "preds = clf.predict_proba(X_test_clean)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's totally true that we could write a few functions to take care of this, but scikit already provides some tooling that end up being cleaner using [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "What is a pipeline? Pretty simple: it's a set of steps that has a model at the end of it. It implements the same API as the models (has `predict` and/or `predict_proba`) but it applies each of the steps before calling the model with the input!\n",
    "\n",
    "Let's see how to use one to simplify the code we just looked at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/titanic.csv')\n",
    "X_train, y_train = train_df.drop('Survived', axis=1), train_df.Survived.copy()\n",
    "X_test = pd.read_csv('data/titanic-test.csv')\n",
    "\n",
    "# for some cases, we will want to create our own pipeline step\n",
    "# this provides a lot of flexibility!\n",
    "class RemoveObjectColumns(TransformerMixin):\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        return X.select_dtypes(exclude='object').copy()\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "# now let's make the pipeline\n",
    "pipeline = make_pipeline(\n",
    "    RemoveObjectColumns(),\n",
    "    # it's cool how scikit already has a mean imputer ready to go!\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    RandomForestClassifier(n_estimators=10)\n",
    ")\n",
    "\n",
    "# No need for us to manually preprocess the X_train at all\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Same thing for X_test\n",
    "probas = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can take a bit of time to learn how to use and they do have some strange behavior in some cases so be a bit patient with them - the work pays off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Estimator\n",
    "\n",
    "You may have noticed that the APIs that we use in our models are very simple. There is a `fit()`, a `predict`, a `predict_proba`, and sometimes a `transform` that we use and really not much else. If you're thinking that you can create your own estimator without much trouble, you'd be 100% correct!\n",
    "\n",
    "#### There are good docs for this\n",
    "\n",
    "Check out the section of the scikit user guide called [rolling your own estimator](https://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator) for the official explanations\n",
    "of exactly how to do this. One of the more useful things that these docs have are a pointer to a\n",
    "repo that has [project templates](https://github.com/scikit-learn-contrib/project-template/) which\n",
    "includes several examples of some [custom estimators](https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py)\n",
    "\n",
    "#### Let's roll our own model\n",
    "\n",
    "Alright, let's consider a binary classifier that does something really stupid: it flips a coin for\n",
    "each observation and assigns a class based upon the outcome.\n",
    "\n",
    "For custom estimators, there are normally the following steps:\n",
    "\n",
    "1. Create a class from the `BaseEstimator` and optionally a `ClassifierMixin`.\n",
    "1. Implement a constructor with your hyperparams.\n",
    "1. Implement the `fit()` method.\n",
    "1. Implement a `predict()` method.\n",
    "\n",
    "Consider the following (taken directly from the scikit documentation) that implements a classifier based upon\n",
    "a 1-NN scheme. In this classifier, we are given a set of obsevations with labels and when we get a new one,\n",
    "we just find the sample from the training data that is closest to it and mark it as the same label. Simple :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" An example classifier which implements a 1-NN algorithm.\n",
    "    For more information regarding how to build your own classifier, read more\n",
    "    in the :ref:`User Guide <user_guide>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    demo_param : str, default='demo'\n",
    "        A parameter used for demonstation of how to pass and store paramters.\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_ : ndarray, shape (n_samples, n_features)\n",
    "        The input passed during :meth:`fit`.\n",
    "    y_ : ndarray, shape (n_samples,)\n",
    "        The labels passed during :meth:`fit`.\n",
    "    classes_ : ndarray, shape (n_classes,)\n",
    "        The classes seen at :meth:`fit`.\n",
    "    \"\"\"\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"A reference implementation of a fitting function for a classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target values. An array of int.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" A reference implementation of a prediction for a classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The label for each sample is the label of the closest sample\n",
    "            seen during fit.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.y_[closest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some advice for working in hackathon teams\n",
    "\n",
    "### General advice\n",
    "\n",
    "- Aim to make a submission as early as possible (baseline model)\n",
    "- During the EDA, make sure to output some plots and save them - they will be helpful to build your presentation\n",
    "- Try to keep a \"pipeline\" for your code, from the beginning to the end. Do not rely on successively edit the same DataFrame object, or you might end up unable to re-try to run your code. (I remember we suffered a lot on our first hackaton because of this) (edited) \n",
    "\n",
    "\n",
    "### Advice for working in teams\n",
    "\n",
    "How to split work: should everyone work on their own notebooks? should you keep a single notebook? what is the best strategy?\n",
    "\n",
    "Our advice is to keep a \"main\" notebook that everyone has access to. Nominate a \"guardian\" of such notebook. Work locally on small problems, starting from the \"main\" notebook  - make sure that everyone on the team knows which problem you are attacking. Once you are happy with the solution, add it to the main notebook and make sure everyone knows it has been updated.\n",
    "\n",
    "Also, set time deadlines for tasks. For instance (\"now, everyone has 40 minutes to explore these variables, and we talk again afterwards to share our findings\"). Time goes by fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "Keep this notebook open and reference it regularly, especially when you are doing your first few hackathons. The first few times you get a new dataset and it's 100% up to you to make all decisions about the steps to take\n",
    "it will be VERY easy to skip important steps which will lead you to have much less fun than you deserve!\n",
    "\n",
    "For lots of other additional advice on how to organize your code in your notebooks, check out the Examples notebook\n",
    "that has lots of tips mostly focused around writing well-organized code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
