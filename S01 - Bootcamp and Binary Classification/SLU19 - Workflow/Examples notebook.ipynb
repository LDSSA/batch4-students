{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even if you don't use an import until way later on in the file, put your imports here!\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples and best practices\n",
    "\n",
    "This examples notebook contains some supplementary material to the Learning notebook. Most of the additional material is just for helping you write better code in your Jupyter notebooks so that you spend more time doing science and less time debugging code. Take these suggestions seriously and you'll be able to experiment with more things than someone who litters their notebooks with spaghetti code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can take a bit of time to learn how to use and they do have some strange behavior in some cases so be a bit patient with them - the work pays off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use well-named variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty straightforward - you want to be able to read your code, right? With poorly-named variables, you won't nee need much time before you forget what the variables mean (to say nothing of other people reading your code). \n",
    "\n",
    "Take this for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('data/titanic.csv')\n",
    "b, c = a.drop('Survived', axis=1), a.Survived.copy()\n",
    "d = pd.read_csv('data/titanic-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so we have variables `a`, `b`, `c` and `d`, and `a` is the training set, and `c` is your target variable. Alright, cool-cool-cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_clean = b.select_dtypes(exclude='object').copy()\n",
    "\n",
    "# note that you will want to impute with the median age from the training set\n",
    "# and NOT the test set. This creates a few difficultites when trying to design\n",
    "# around it\n",
    "b_clean['Age'] = b_clean.Age.fillna(b_clean.Age.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we've taken care of `b`, and now we have a `b_clean`. Makes sense. Right, so now let's fit our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = RandomForestClassifier(n_estimators=10)\n",
    "d.fit(b_clean, c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do some predictions! Wait... What? Where's our test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed straight-away, since this was a rather simple example, but imagine you had a few more variables. Things get... tricky. Save yourself some headaches, and a lot of time. Name your variables appropriately from the beginning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use functions!\n",
    "\n",
    "Don't be lazy with duplicating code. It is almost never worth it! The 5 minutes you will save by not taking common functionality and putting it into a function is almost never worth it because the confusion that comes afterward will always bite you.\n",
    "\n",
    "Let's take a real-life example from SLU17 in which we need to read a dataframe and get the dummies. However, sometimes we will want to drop columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the first case where we don't need to drop any columns\n",
    "\n",
    "_df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "# Split the factors and the target\n",
    "X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "# take special note of this call!\n",
    "X = pd.get_dummies(X, dummy_na=True).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we remember, we decided that there were useless features so we wanted to drop some. One way we could achieve this is to copy-paste the cell into another one and then modify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "##################\n",
    "# NEW CODE\n",
    "\n",
    "categoricals = [\n",
    "    'PassengerId',\n",
    "    'Name',\n",
    "    'Ticket',\n",
    "    'Sex',\n",
    "    'Cabin',\n",
    "    'Embarked',\n",
    "]\n",
    "drop_columns = [\n",
    "    'Ticket',\n",
    "    'PassengerId'\n",
    "]\n",
    "\n",
    "# now drop any columns that are specified as needing to be dropped\n",
    "for colname in drop_columns:\n",
    "    _df = _df.drop(colname, axis=1)\n",
    "# END NEW CODE\n",
    "##################\n",
    "\n",
    "for colname in categoricals:\n",
    "    if colname in drop_columns:\n",
    "        continue\n",
    "    _df[colname] = _df[colname].fillna('null').astype('category')\n",
    "\n",
    "\n",
    "# Split the factors and the target\n",
    "X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "# take special note of this call!\n",
    "X = pd.get_dummies(X, dummy_na=True).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes my eyes bleed! Arrrggghhh it hurts! Why is it so painful? Imagine that you want to change the `fillna(-1)` to `fillna(0)`. You now have to change this in two places. Now imagine that you have duplicated this cell in 10 different places, you're going to have to remember to change this in 10 different places and you're going to have a terrible time.\n",
    "\n",
    "What's the right thing to do here? Put it in a function! It doesn't have to be anything special and as you all passed the test, we know you all know how to write functions so NO EXCUSES. Here's what my function for this ended up looking like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_get_dummies(drop_columns=[]):\n",
    "    # when working inside of functions, always call your dataframe\n",
    "    # _df so that you know you're never using any from the outside!\n",
    "    _df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "    # now drop any columns that are specified as needing to be dropped\n",
    "    for colname in drop_columns:\n",
    "        _df = _df.drop(colname, axis=1)\n",
    "    \n",
    "    for colname in categoricals:\n",
    "        if colname in drop_columns:\n",
    "            continue\n",
    "        _df[colname] = _df[colname].fillna('null').astype('category')\n",
    "\n",
    "\n",
    "    # Split the factors and the target\n",
    "    X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "    # take special note of this call!\n",
    "    X = pd.get_dummies(X, dummy_na=True).fillna(-1)\n",
    "\n",
    "    return _df, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which would take the previous two cells to this niceness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, X, y = read_and_get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "    'Ticket',\n",
    "    'PassengerId'\n",
    "]\n",
    "df_train, X, y = read_and_get_dummies(drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, huge improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When writing functions\n",
    "\n",
    "1. If you take a parameter that is a dataframe, always name if `_df`\n",
    "1. Outside of functions, never name a dataframe `_df`\n",
    "1. Inside of functions, never name a dataframe `df`\n",
    "\n",
    "Why you ask? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you're working in a notebook with the following dataframe\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'f_1': [1, 2, 3, 4],\n",
    "    'f_2': [4, 5, 6, 7],\n",
    "    'age': [50, 60, 70, 80],\n",
    "    'sex': ['m', 'f', 'f', 'm']\n",
    "})\n",
    "\n",
    "# say you have a convenience function to drop all object columns\n",
    "def drop_cats(df):\n",
    "    return df.drop(df.select_dtypes(include=['object']).columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you make another version of the function because you remember to name the dataframe parameter `_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_and_dogs = pd.DataFrame({\n",
    "    'cat': ['hello'],\n",
    "    'dog': [1]\n",
    "})\n",
    "\n",
    "def drop_cats(_df):\n",
    "    return df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
    "\n",
    "drop_cats(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah this was not expected! Easy fix though: when working on functions, always use `_df` from the very beginning when inside of the function and then never use `_df` as a variable name outside of the function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutability is key!\n",
    "\n",
    "If you pass a dataframe into a function, NEVER EVER EVER will you want to change the dataframe that was passed in. This causes chaos in the world. A few rules to adhere to when doing this are the following:\n",
    "\n",
    "1. Never use inplace=True\n",
    "1. If you have a function that takes a dataframe as an argument and returns a modified version of it, always copy it at the beginning of the function and make your changes to that copy.\n",
    "\n",
    "Why? Let's see an example.\n",
    "\n",
    "Say I want a function that drops all columns that start with 'f_':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_f(_df):\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    for colname in _df.columns:\n",
    "        if colname.startswith('f_'):\n",
    "            cols_to_drop.append(colname)\n",
    "\n",
    "    _df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'f_1': [1, 2, 3, 4],\n",
    "    'f_2': [4, 5, 6, 7],\n",
    "    'age': [50, 60, 70, 80],\n",
    "    'sex': ['m', 'f', 'f', 'm']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's drop the f_\n",
    "df_no_f = drop_f(df)\n",
    "df_no_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great success! Now let's check out the original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, we never said to drop it from the original dataframe but it happened anyway! What the heck?\n",
    "\n",
    "That's because we violated the rules and use inplace=True nor did we make a copy! Let's rewrite this function to be a bit nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_f(_df):\n",
    "    \n",
    "    _df = _df.copy()\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    for colname in _df.columns:\n",
    "        if colname.startswith('f_'):\n",
    "            cols_to_drop.append(colname)\n",
    "\n",
    "    _df = _df.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'f_1': [1, 2, 3, 4],\n",
    "    'f_2': [4, 5, 6, 7],\n",
    "    'age': [50, 60, 70, 80],\n",
    "    'sex': ['m', 'f', 'f', 'm']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_f = drop_f(df)\n",
    "df_no_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!\n",
    "\n",
    "Now at this point you might be saying: why not just pick one of the two rules, either of those would have worked. That's totally true, in this case. However, all good programmers should adhere to best practices and the problem with `inplace=True` is that it changes the `drop()` API by making it return `None`.\n",
    "\n",
    "This WILL CAUSE CONFUSION! If you are used to using `inplace=True` and another person you are working with is not used to using it, I can guarantee that you'll copy the person's code at least once and then wonder why this doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'f_1': [1, 2, 3, 4],\n",
    "    'f_2': [4, 5, 6, 7],\n",
    "    'age': [50, 60, 70, 80],\n",
    "    'sex': ['m', 'f', 'f', 'm']\n",
    "})\n",
    "no_f = df.drop('f_1', inplace=True, axis=1)\n",
    "no_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is for these reason as well as many others that immutability is preferable no matter what language you're working in. Trust me on this one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports up top!\n",
    "\n",
    "You should always have your imports up at the top of the file. Anyone that looks at your notebook should get an idea of the tools that you are using IMMEDIATELY with a glance as soon as opening your notebook.\n",
    "\n",
    "And help your own sanity as well! You don't want to loose a key import during a cleanup and then when someone fires up your notebook for the first time and runs it, there's an import missing. This happens ALL the time, so you need to minimize as much as possible these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize your directory\n",
    "\n",
    "1. All your notebooks should be in the same directory.\n",
    "1. You should create a data directory and put all of your data in it\n",
    "1. If you have images or other media, put it in a media directory.\n",
    "\n",
    "Here is an example directory structure from an SLU\n",
    "\n",
    "<img src=\"media/directory-structure.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When writing functions\n",
    "\n",
    "1. Keep them close to the cell where you are testing it at first\n",
    "1. When they are stabilizing, move them to the top of the notebook\n",
    "1. When you are starting to use them a lot, move them into a utils.py file and import from there\n",
    "\n",
    "You can see many examples of this in the files in this repository, follow them!\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'int_col': [1, 2, 3],\n",
    "    'float_col': [1.0, 2.0, 3.0],\n",
    "    'cat_col_1': pd.Series(['a', 'b', 'c'], dtype='category'),\n",
    "    'cat_col_2': pd.Series(['d', 'e', 'f'], dtype='category')\n",
    "})\n",
    "\n",
    "df_medium = pd.DataFrame({\n",
    "    'int_col': [1, 2, 3],\n",
    "    'float_col': [1.0, 2.0, 3.0],\n",
    "    'cat_col_1': pd.Series(['a', 'b', 'c'], dtype='category'),\n",
    "    'cat_col_2': pd.Series(['d', 'e', 'f'], dtype='category'),\n",
    "    'last_col': [9, 10, 00]\n",
    "})\n",
    "\n",
    "df_full = pd.DataFrame({\n",
    "    'int_col': [1, 2, 3],\n",
    "    'float_col': [1.0, 2.0, 3.0],\n",
    "    'cat_col_1': pd.Series(['a', 'b', 'c'], dtype='category'),\n",
    "    'cat_col_2': pd.Series(['d', 'e', 'f'], dtype='category'),\n",
    "    'int_col_2': [1, 2, 3],\n",
    "    'float_col_2': [1.0, 2.0, 3.0],\n",
    "    'cat_col_3': pd.Series(['a', 'b', 'c'], dtype='category'),\n",
    "    'cat_col_4': pd.Series(['d', 'e', 'f'], dtype='category')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need a new function called get_categoricals\n",
    "\n",
    "def get_categoricals(_df):\n",
    "    return _df.select_dtypes(include=['category'])\n",
    "\n",
    "# and I'm going to use it here\n",
    "\n",
    "get_categoricals(df)\n",
    "get_categoricals(df_full)\n",
    "get_categoricals(df_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've finished with the function and it's all tested, I need to move the function up to the top of the file so that the cell where you are doing the actual work is much cleaner. Now this is only with a single-line function and you can already see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much cleaner, right?\n",
    "cat_columns = get_categoricals(df)\n",
    "cat_columns_full_dataset = get_categoricals(df_full)\n",
    "cat_columns_medium_dataset = get_categoricals(df_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I do this enough times, I'll get a notebook call that contains my functions up top that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\n",
    "    'PassengerId',\n",
    "    'Name',\n",
    "    'Ticket',\n",
    "    'Sex',\n",
    "    'Cabin',\n",
    "    'Embarked',\n",
    "]\n",
    "\n",
    "# Here are a few functions that we will use to do our proof of concepts\n",
    "# around training and testing for kaggle submissions for the titanic\n",
    "# competition.\n",
    "\n",
    "# The reason that we need to have these functions they way they are\n",
    "# is to manage the preprocessing of the features the same for the \n",
    "# training and test set. Don't worry too much about exactly what's\n",
    "# going on in these functions right now but rather focus on the concepts\n",
    "# that are being covered after this in the notebook.\n",
    "\n",
    "\n",
    "def read_and_get_dummies(drop_columns=[]):\n",
    "    # when working inside of functions, always call your dataframe\n",
    "    # _df so that you know you're never using any from the outside!\n",
    "    _df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "    # now drop any columns that are specified as needing to be dropped\n",
    "    for colname in drop_columns:\n",
    "        _df = _df.drop(colname, axis=1)\n",
    "    \n",
    "    for colname in categoricals:\n",
    "        if colname in drop_columns:\n",
    "            continue\n",
    "        _df[colname] = _df[colname].fillna('null').astype('category')\n",
    "\n",
    "\n",
    "    # Split the factors and the target\n",
    "    X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "    # take special note of this call!\n",
    "    X = pd.get_dummies(X, dummy_na=True).fillna(-1)\n",
    "\n",
    "    return _df, X, y\n",
    "    \n",
    "\n",
    "\n",
    "def train_and_test(drop_columns=[], max_depth=None, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Train a decision tree and return the classifier, the X_train,\n",
    "    and the original dataframe so that they can be used on the test\n",
    "    set later on.\n",
    "    \"\"\"\n",
    "    \n",
    "    _df, X, y = read_and_get_dummies(drop_columns=drop_columns)\n",
    "    \n",
    "    # Now let's get our train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=1, test_size=test_size)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    score = accuracy_score(y_test, clf.predict(X_test))\n",
    "    print('X_test accuracy {}'.format(score))\n",
    "    print('X_train shape: {}'.format(X_train.shape))\n",
    "    \n",
    "    return X_train, _df, clf\n",
    "\n",
    "\n",
    "def produce_test_predictions(train_df, clf, drop_columns=[]):\n",
    "    _df = pd.read_csv('data/titanic-test.csv')\n",
    "    \n",
    "    for colname in drop_columns:\n",
    "        _df = _df.drop(colname, axis=1)\n",
    "        \n",
    "    for colname in categoricals:\n",
    "        if colname in drop_columns:\n",
    "            continue\n",
    "        _df[colname] = _df[colname].fillna('null')\n",
    "        _df[colname] = pd.Categorical(\n",
    "            _df[colname],\n",
    "            categories=train_df[colname].cat.categories\n",
    "        )\n",
    "        \n",
    "    X = pd.get_dummies(_df, dummy_na=True).fillna(-1)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'PassengerId': pd.read_csv('data/titanic-test.csv').PassengerId,\n",
    "        'Survived': pd.Series(clf.predict(X))\n",
    "    })\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Test Set score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAAAH! It's too much! I have to scroll for a mile just to get to the first informative part of the notebook! \n",
    "\n",
    "At this point, move it into a `utils.py` file\n",
    "\n",
    "## Caveat - changes to utils.py require a reload\n",
    "\n",
    "This is a huge bummer: jupyter will not automatically pick up on changes to your utils.py\n",
    "file without being told to reload them. This means that you should use the autoreload. The way you should use it is like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above your import cell, do this\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from IPython.display import Image\n",
    "# from utils import say_hi\n",
    "\n",
    "# them in your import cell, include the %autoreload 2 at the bottom of it\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if you make changes to utils.py you just need to re-run the import cell and you are good to go with the updated versions of all your functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periodically restart your notebooks\n",
    "\n",
    "Because notebooks are continuously running processes, it's REALLY easy to leave stuff lying around that you can't see any more. I can't really show you an example in this notebook because it would require a live demo but let's do a little thought experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1 of this cell\n",
    "a_var = 10\n",
    "b_var = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say that I don't need the `b_var` any more so I delete it from the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 of this cell (imagine that I changed the original one)\n",
    "a_var = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are left with a notebook that doesn't have the `b_var` anywhere in it. However when I execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still here! Now this is sloppy and unfortunate and a perfect example of why jupyter notebook is not an IDE. `b_var` does not exist anywhere in the notebook yet if I reference it, it still exists!\n",
    "\n",
    "The solution? Restart the notebook! If you do this and then run the `b_var` cell again, it will error as it should:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart and re-run the entire notebook before calling it done\n",
    "\n",
    "This is a huge one! Because of un-intended side effects of how jupyter works, it is inevitable that different cells will have been run with different versions of the notebook. If you have a cell working with an old version of a dataframe or function it could be displaying the wrong output. Then if you deliver this to someone, and they re-run it without changing ANY code, you're going to run into problems.\n",
    "\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_hello():\n",
    "    print('whaddap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the function and run it again in a different cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that if someone comes along and sees this they will be MASSIVELY confused. It's easy to fix this, just restart the notebook and run everything again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling large datasets on your machine\n",
    "\n",
    "Okay say that your working with a fairly large dataset and it's taking a while to train. Furthermore may the full dataset doesn't fit into memory on your machine but you still want to do some EDA as well as train a model. There are a few ways to do this\n",
    "\n",
    "1. Only read part of the file into memory\n",
    "1. If you are able to read the whole thing into memory but don't want to be working with it the entire time, you can take a sample of it while doing your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only read the first 10 rows\n",
    "pd.read_csv('data/titanic.csv', nrows=10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the whole thing into memory but then sample only a portions of it\n",
    "# this is okay if you are able to read the whole thing into memory and just\n",
    "# want to speed up training or something like that \n",
    "pd.read_csv('./data/titanic.csv').sample(frac=.10).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to take a random sample from what is actually on the disk, there are a few ways to do this\n",
    "and none of them are really great. Here are some suggestions if you ever run into this\n",
    "\n",
    "https://stackoverflow.com/questions/22258491/read-a-small-random-sample-from-a-big-csv-file-into-a-python-data-frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOST IMPORTANT LESSON\n",
    "\n",
    "If you are experimenting with building a classifier for a dataset, you should make it as easy as possible for you to perform an experiment. What this means in terms of a jupyter notebook is...\n",
    "\n",
    "## Each experiment should be runnable in a single cell\n",
    "\n",
    "If you want to create a new experiment, you should be able to copy-paste a small amount of code into a new cell.\n",
    "\n",
    "If you change something in an experiment, you should be able to re-run the WHOLE experiment by running one cell.\n",
    "\n",
    "This is not about correctness of your models or your analysis. You can have a completely disorganized notebook that produces the exact same one that is half the size and very well organized. However, due to avoiding unnecessary technical debt, the organized notebook will be able to perform more experiments more quickly.\n",
    "\n",
    "Take SLU17 as an example. Here it is with everything spread across many cells and nothing being in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv('data/titanic.csv')\n",
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\n",
    "    'PassengerId',\n",
    "    'Name',\n",
    "    'Ticket',\n",
    "    'Sex',\n",
    "    'Cabin',\n",
    "    'Embarked',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_columns = [\n",
    "    'Ticket',\n",
    "    'PassengerId'\n",
    "]\n",
    "\n",
    "# now drop any columns that are specified as needing to be dropped\n",
    "for colname in drop_columns:\n",
    "    _df = _df.drop(colname, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in categoricals:\n",
    "    if colname in drop_columns:\n",
    "        continue\n",
    "    _df[colname] = _df[colname].fillna('null').astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the factors and the target\n",
    "X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take special note of this call!\n",
    "X = pd.get_dummies(X, dummy_na=True).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This might seem like a good idea\n",
    "\n",
    "Being verbose can be good in some cases such as when you're teaching someone how to do something step-by-step. However it's not a good idea when you need to run a tight experiment! Take all of the above and put it into a single cell and look at how much more tidy it becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "drop_columns = [\n",
    "    'Ticket',\n",
    "    'PassengerId'\n",
    "]\n",
    "\n",
    "# now drop any columns that are specified as needing to be dropped\n",
    "for colname in drop_columns:\n",
    "    _df = _df.drop(colname, axis=1)\n",
    "\n",
    "for colname in categoricals:\n",
    "    if colname in drop_columns:\n",
    "        continue\n",
    "    _df[colname] = _df[colname].fillna('null').astype('category')\n",
    "\n",
    "# Split the factors and the target\n",
    "X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "# take special note of this call!\n",
    "X = pd.get_dummies(X, dummy_na=True).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you need to change something, you have the entire visibility of what you are doing in one place and once you make your change, you only have to run a single cell to re-run all steps in your experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
