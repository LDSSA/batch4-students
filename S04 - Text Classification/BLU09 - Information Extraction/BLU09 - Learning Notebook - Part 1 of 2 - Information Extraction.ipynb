{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Information Extraction\n",
    "\n",
    "Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. Information Extraction may be presented in three subtasks:\n",
    "\n",
    "* **Named Entity Recognition**, retrieve entities (like persons, location, etc.) in the text. \n",
    "* **Relation Extraction**, find the relation between two entities in the text.\n",
    "* **Template Filling**, find the correct entity to fill a certain template, for instance.\n",
    "\n",
    "In this BLU we are going to learn some of the basic techniques to extract specific (pre-specified) information from textual sources. From the three specified tasks, we are going to **focus on the task of named-entity recognition (NER)** where our objective is to **retrieve all the mentions** of entities like persons, locations, time, among others. The other two are mentioned for the sake of completeness and you should definitely research more about them, specially if you're eager to learn more about NLP.\n",
    "\n",
    "![robot entities](./media/robot_entities.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on a corpus containing forum discussions. We extracted a sample from Reddit for this use. For more interesting examples, you may find more textual data available at https://files.pushshift.io/reddit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: setuptools in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/minh/.virtualenvs/blu09/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read 1000 documents\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('./datasets/sample_data.json') as fp:\n",
    "    for line in fp:\n",
    "        entry = json.loads(line)\n",
    "        docs.append(entry['body'])\n",
    "        \n",
    "print('I read {} documents'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction with Regular Expressions\n",
    "\n",
    "In BLU07, we became pros of regular expressions. We're going to try to use them to our task of recognizing entities. Take a moment to think about all the possibilities of Entities that we can find in a text. Do you think such a task will be achievable using only regular expressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regex](./media/regex.gif \"regex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, let's say that your boss asked you to retrieve all the **dates** mentioned in our sample corpus. We learned in BLU7 that it is easy to use a regular expression for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14/09/30', '7/12/2007', '4/16/2007', '3/27/2007', '2/28/2007']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find all possible dates in the format xx/xx/xxxx\n",
    "data = ' '.join(docs)\n",
    "re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks like it's going to be a breeze. However, now your boss decides to ask you to retrieve all the **country names** which appear in the corpus instead. \n",
    "\n",
    "One possible approach is to retrieve a list of all countries that exist and look for the occurence of such elements in the corpus. Let's try that, shall we?\n",
    "\n",
    "![alt text](./media/countries_meme.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "with open('./datasets/countries.txt') as fp:\n",
    "    for line in fp:\n",
    "        countries.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use again regular expressions for this. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('us', 763, 765)\n",
      "('United States', 827, 840)\n",
      "('UK', 6971, 6973)\n",
      "('US', 7000, 7002)\n",
      "('Puerto rico', 8026, 8037)\n",
      "('us', 8638, 8640)\n",
      "('France', 19815, 19821)\n",
      "('us', 21563, 21565)\n",
      "('Puerto Rico', 27659, 27670)\n",
      "('Puerto Rico', 27754, 27765)\n",
      "('US', 28101, 28103)\n",
      "('Canada', 29439, 29445)\n",
      "('USA', 32880, 32883)\n",
      "('Norway', 34749, 34755)\n",
      "('Korea', 34837, 34842)\n",
      "('USA', 35738, 35741)\n",
      "('United States', 41060, 41073)\n",
      "('us', 42290, 42292)\n",
      "('us', 42403, 42405)\n",
      "('Soviet', 44563, 44569)\n",
      "('us', 49625, 49627)\n",
      "('Chad', 51352, 51356)\n"
     ]
    }
   ],
   "source": [
    "# Sort country list by length. This is important to match longer spans before short \n",
    "# ones (like in 'Papua New Guinea' vs. 'Papua')\n",
    "countries.sort(key=len, reverse=True)\n",
    "\n",
    "# Make a regex to recognize all possible names.\n",
    "# '|' creates the or operation in regex\n",
    "# \\b means word boundaries (punctuation or white spaces)\n",
    "# re.escape is used to escape regex operators like '.'    \n",
    "countries_regex = r'\\b(' + '|'.join([re.escape(c) for c in countries]) + r')\\b'\n",
    "\n",
    "# finditer is similar to findall\n",
    "# the flag re.I means to ignore casing (accept both lowercase and uppercase letters as the same)\n",
    "for i, m in enumerate(re.finditer(countries_regex, data, flags=re.I)):\n",
    "    print( (m.group(), m.start(), m.end()) )\n",
    "    # just show the first 20\n",
    "    if i > 20:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this approach working?**\n",
    "\n",
    "It seems like the word **'us'**, for example, has caused some confusion. It could be the country _U.S._, or just the pronoun _us_. In this case, just comparing the word form we are not able to disambiguate the two forms. We will need either more **context** or more **linguistic information** and regular expressions won't give us none of that.\n",
    "\n",
    "Luckily, you already know an NLP library which can provide you the correct information to disambiguate the word 'us'. In the next examples, we will use SpaCy as our NLP toolkit to give us just that.\n",
    "\n",
    "## Deeper look into information extraction using SpaCy\n",
    "![Spacy](./media/spacy.jpg)\n",
    "\n",
    "If you remember BLU08, we used SpaCy to understand word vectors (aka word embeddings). We will make use of the medium sized SpaCy english model once again. In case you haven't downloaded it yet, you can simply use this command in a Notebook cell:\n",
    "\n",
    "```\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n",
    "    \n",
    "But of course we could have used any english model (en_core_web_sm, en_core_web_md, en_core_web_lg) provided by SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are disabling the synctatic parser from pipeline to improve speed.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SpaCy, we will process the documents with the complete NLP pipeline using [pipe](https://spacy.io/usage/processing-pipelines). This means that `pipe` will process our text, tokenize it and extract information from it using all the CPU cores from our machine. Concretely, it will Part-of-Speech tag (more on that later), parse and extract entities.\n",
    "\n",
    "We won't get into details on how SpaCy does this -- what matters is that it uses fast machine learning models with good enough accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use the function pipe to process all documents.\n",
    "# One of the strenghts for SpaCy is the parallel processing using all your computer cores.\n",
    "# In this step, SpaCy performs the NLP pipeline for all the docs, so it may take a while\n",
    "\n",
    "docs = list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to do NER (Named Entity Extraction) in a piece of text. We can get an example sentence from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was more like $10 million dollars after the previous government infringed on his rights within the Charter of Rights and Freedoms (basically the Canadian constitution). Trudeau knew Khadr would win in court, and settled for paying $10 million instead of an amount multiple times more. \n",
      "\n",
      "Not saying I'm happy with Khadr getting $10 million, but this was more of a fuck up on the previous government since they blatantly violated his rights as a Canadian.\n"
     ]
    }
   ],
   "source": [
    "example = docs[250]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SpaCy, it's really easy to extract entities - we can simply use `.ents` in our previously processed text, and SpaCy will use its built-in model to get the entities present in the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like $10 million dollars 12 36 MONEY\n",
      "the Charter of Rights and Freedoms 98 132 LAW\n",
      "Canadian 148 156 NORP\n",
      "Trudeau 172 179 PERSON\n",
      "Khadr 185 190 PERSON\n",
      "$10 million 234 245 MONEY\n",
      "Khadr 316 321 PERSON\n",
      "$10 million 330 341 MONEY\n",
      "Canadian 447 455 NORP\n"
     ]
    }
   ],
   "source": [
    "for ent in example.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example sentence, SpaCy gave correct labels for our examples, such as Trudeau and Khadr are PERSON, and Canadian is NORP (Nationalities or religious or political groups). You can find here the complete [list of different entity types](https://spacy.io/api/annotation#named-entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is processed and we know how to get entities, let's build a `Matcher` in SpaCy.\n",
    "\n",
    "A `Matcher` is SpaCy's version of a regular expression - it searches for patterns in your text, according to the rules you give it. However, it is much more powerful since it has access to the outputs of the aforementioned NLP pipeline. That means we can search patterns that include certain entities or Part-of-Speech tags. \n",
    "\n",
    "In this `Matcher` we will define templates which we will use later to match elements in the text (thus using it to do information extraction). The `Matcher` is initialized using the vocabulary object, which must be shared with the documents the matcher will operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # Pass the vocabulary object to Matcher.__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a similar matcher as we did above with regular expressions. We are going to get each country name and add it as a pattern to the `matcher`. To add a pattern, we can simply use `.add()`. It receives:\n",
    "\n",
    "- an ID (the name we want to give our pattern)\n",
    "- a callable function that is called when there is a match (we're not going to use anything)\n",
    "- the pattern itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # Build a pattern from the country name. For example: United States -> [{'LOWER': 'united'}, {'LOWER': 'states'}]\n",
    "    # LOWER means to match the words in the lowercased token.\n",
    "    pattern = [{'LOWER': c.lower()} for c in country.split()]\n",
    "    matcher.add(country, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 2 us\n",
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "69 50 51 us\n",
      "146 4 5 France\n",
      "167 29 30 us\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "320 35 36 us\n",
      "320 58 59 us\n",
      "335 38 39 Soviet\n",
      "335 38 39 Soviet\n",
      "349 4 5 us\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "# for screen economy, let's just show the matches for the first 400 documents.\n",
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, in order to disambiguate the retrieval of 'U.S.' vs 'us' we need to add more linguistic information to the `matcher`. Let's play with Part of Speech (PoS).\n",
    "\n",
    "## But what is Part-of-Speech?\n",
    "\n",
    "If you remember from your language classes, you could categorize words in a sentence according to the role they have in it. In NLP, we call this Part of Speech tags. For the English language, common PoS tags are: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection.\n",
    "\n",
    "SpaCy adopts the Universal PoS tagset where any language has a common subset of PoS defined. The list of all possible values can be consulted [here](https://spacy.io/api/annotation#pos-tagging).\n",
    "\n",
    "In this case, we are interested in matching the country names that were tagged as **Proper Nouns** ('PROPN' tag obtained from the tagset list).\n",
    "\n",
    "![Pronoun meme](./media/pronoun.jpg)\n",
    "\n",
    "In SpaCy, just as entities of a document are inside `doc.ents`, for each token of a document we can find its assigned POS tag by using `.pos_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR PROPN\n",
      "Tolkien PROPN\n",
      ". PUNCT\n",
      "Gandalf PROPN\n",
      ", PUNCT\n",
      "Aragorn PROPN\n",
      ", PUNCT\n",
      "Frodo PROPN\n",
      ", PUNCT\n",
      "Bilbo PROPN\n",
      "Baggins PROPN\n",
      ", PUNCT\n",
      "Gollum PROPN\n",
      "... PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in example:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `Matcher` is pretty smart, so we only really need to add to a `'POS'` entry in the pattern dictionary and the tag we are looking for as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new matcher instance\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for country in countries:\n",
    "    # same as before, but now with one more restriction: the Part-of-speech should be a Pronoun.\n",
    "    pattern = [{'LOWER': c.lower(), 'POS': 'PROPN'} for c in country.split()]    \n",
    "    matcher.add(country, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 2 us\n",
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "146 4 5 France\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start:end]\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the PoS tagger is based on a machine learning method, so it is prone to errors. Notice how it causes _Puerto rico_ of document 64 to be out of this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting using complex patterns\n",
    "\n",
    "Let's now look into other types of information extraction methods which use complex structures. For example, let's say we want to extract places. Usually, places come up in text in structures similar to:\n",
    "\n",
    "* go to xx\n",
    "* went from xxx\n",
    "* going to xx\n",
    "\n",
    "**Note**: Notice that such patterns could be interesting to the task of relation extraction we mentioned in the intro. But that's something we will leave up to you to look further into.\n",
    "\n",
    "In order to build a SpaCy pattern for the proposed sentence structure, we are going to use the lemma word 'go' (remember lemmatization from BLU07? We can do this in SpaCy pretty easily as well!), which is invariant for all possible verb inflections, a preposition (POS tag name - ADP) and a proper noun (POS tag name - PROPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'LEMMA': 'go'}, {'POS': 'ADP'}, {'POS': 'PROPN'}]\n",
    "matcher.add('LOC', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 5 go for Sorcery\n",
      "24 27 goes to GTA\n",
      "246 249 going to Osaka\n",
      "81 84 gone to Irvine\n",
      "91 94 going with Robbie\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        print(start, end, span_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sure aren't all the locations that are present in our corpus! Not what we expected then :( \n",
    "\n",
    "Once again, we are finding out that it is very difficult to build patterns to match these type of ocurrences in the text. Addressing all possible patterns for person, location, etc. this way is very inneficient and difficult. \n",
    "\n",
    "Another possible way to go is to annotate examples in a corpus. We can train machine learning systems to automatically extract patterns from annotated corpora. Such class of machine learning methods are known as sequencial labeling and the most famous approaches are [CRFs](https://people.cs.umass.edu/~wallach/technical_reports/wallach04conditional.pdf) and [Seq2seq](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).\n",
    "\n",
    "Fortunately, as explained above, Spacy already contains pre-trained models for standard named-entities. Besides _Person_ (PER) entities like _Bilbo_ and _Organization_ (ORG) entities like _PayPal_ , we can also extract _Location_ entities with the code GPE!\n",
    "\n",
    "Let's try to extract all Locations using the built-in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 New York 0 8\n",
      "12 United States Anonymous 16 39\n",
      "30 Portland 5 13\n",
      "30 Portland 402 410\n",
      "49 Hoover 26 32\n",
      "58 UK 117 119\n",
      "58 US 146 148\n",
      "64 Puerto rico 81 92\n",
      "71 Arpaio 140 146\n",
      "76 Muana 44 49\n",
      "83 None Anonymous 16 30\n",
      "112 Ixalan 176 182\n",
      "146 France Anonymous 16 32\n",
      "202 Tennessee 107 116\n",
      "213 Puerto Rico 467 478\n",
      "213 Puerto Rico 562 573\n",
      "213 US 909 911\n",
      "218 Neyland 11 18\n",
      "255 anti-USA 378 386\n",
      "262 Cena 412 416\n",
      "262 Lesnar 627 633\n",
      "263 Norway 325 331\n",
      "263 California 363 373\n",
      "263 Korea 413 418\n",
      "267 USA 9 12\n",
      "303 Texas 21 26\n",
      "305 Bitcoin 321 328\n",
      "312 United States Anonymous 16 39\n",
      "320 here](https://redd.it/6ojo0r 464 492\n",
      "356 Dankquan 29 37\n",
      "366 Villain 421 428\n",
      "367 Chad 30 34\n",
      "369 Chad 49 53\n",
      "378 Pumas 45 50\n",
      "386 United States Anonymous 16 39\n",
      "387 Sonya 189 194\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'GPE':   \n",
    "            print(i, e.text, e.start_char, e.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, let's not forget that, as in any machine learning model, we are also prone to errors in our prediction.\n",
    "\n",
    "Could we train a better model? Sure! Given a good corpus for training and the right tools we could achieve a very high accuracy. However, as this is not the objective of this BLU we are going to leave you some links if you want to learn more about this.\n",
    "\n",
    "https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another handy link - https://spacy.io/usage/linguistic-features - you can find here all kind of features SpaCy can extract for you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
