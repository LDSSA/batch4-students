{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### Introduction\n",
    "\n",
    "As you may have noticed, this set of BLUs will revolve around the topic of Natural Language Processing (NLP). As the name implies, this field is all about the processing and handling of language in such a way that a computer may be able to do useful things with it. There are plenty of tasks and problems around it, namely:\n",
    "\n",
    "- **Speech recognition**: the task of, given a sample of audio, extract the words that are being spoken or even prosody features, for example.\n",
    "- **Natural language generation**: the task of putting computational formulations into actual text, for example, automated generation of labels to images, summarisation of texts and data, creation of dialogue systems, etc.\n",
    "- **Natural language understanding**: the task of getting some meaning out of the data, for instance, recognizing entities in sentences, semantic roles or even classify sentences according to its sentiment, etc or transforming it into something machines can work on (numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the main tasks and areas of research of NLP are:\n",
    "\n",
    "- **Part of Speech tagging**: Determine the role of each word in a given sentence, for instance, if it is an adjective, verb, noun, etc.\n",
    "\n",
    "- **Word Segmentation**: Break continuous text into words.\n",
    "\n",
    "- **Parsing**: Define a tree that represents the grammatical structure of a sentence.\n",
    "\n",
    "- **Machine Translation**: Translate sentences from a source language to a target language automatically.\n",
    "\n",
    "- **Named entity recognition**: Find parts of the text that correspond to certain entities, like names of places, people, companies, etc.\n",
    "\n",
    "- **Question answering**: Given a question in human language, find the most appropriate answer.\n",
    "\n",
    "- **Text to speech**: As the name implies, transform written text into audible human like sounds that correspond to the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these tasks are out of scope of these learning units, but we think that it is important to at least acknowledge that they exist in the realm of NLP. Also, some of these things may seem \"easy\", but when you think about the diversity that exists in terms of languages you start to understand how daunting all these tasks are. For instance, word segmentation may seem like a really easy task. After all, words are separated by spaces or maybe some punctuation. But, if you take a look at Mandarin Chinese, for instance, that's not the case, making that \"heuristic\" no longer universal. And for many of the tasks there are plenty of corner cases, which make this field one of the most challenging but also more rewarding to work on.\n",
    "\n",
    "Throughout these learning units we hope to give you some basic understanding on how to transform text into something useful for us, what are some of the challenges in this field, solve some interesting problems and hopefully make you want to learn more about the topic afterwards!\n",
    "\n",
    "The first part of this BLU goes through some of the fundamental concepts that will be helpful for all the practical tasks that you will need during this month, but also in the future, if you ever need to work with text data. We will start by introducing **regular expressions**, followed by three important concepts in data pre-processing (**tokenization**, **stopwords** and **stemming**). Finally we will see what are **n-grams** and what is an **n-gram model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions (aka Regex)\n",
    "\n",
    "Regular expressions are sequences of characters that allow us to define search patterns. It goes by several rules and is one of the most fundamental and important concepts in computer science regarding working with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheatsheet [\\[1\\]](https://regexr.com/3lvai)\n",
    "\n",
    "`.` - matches any character, except newline.\n",
    "\n",
    "`\\d, \\s \\S` - match digit, match whitespace, not whitespace.\n",
    "\n",
    "`\\b, \\B` - word, not word boundary.\n",
    "\n",
    "`[xyz]` - matches x, y or z.\n",
    "\n",
    "`[^xyz]` - matches anything that is not x, y or z.\n",
    "\n",
    "`[x-z]` - matches a character between x and z.\n",
    "\n",
    "`^xyz$` - `^` is the start of the string, `$` is the end of the string.\n",
    "\n",
    "`\\.` - use escaping to match special characters.\n",
    "\n",
    "`\\t`, `\\n` - matches tab and newline.\n",
    "\n",
    "`x*` - matches 0 or more symbols x.\n",
    "\n",
    "`x+` - matches 1 or more symbols x.\n",
    "\n",
    "`x?` - matches 0 or 1 symbol x.\n",
    "\n",
    "`.?`, `*?`, `+?`, etc - represent non-greedy search. \n",
    "\n",
    "`x{5}` - matches exactly 5 symbols x.\n",
    "\n",
    "`x{5,}` - matches 5 or more symbols x.\n",
    "\n",
    "`x{5, 8}` - matches between 5 and 8 symbols x.\n",
    "\n",
    "`xy|yz` - matches `xy` or `yz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use python's [re](https://docs.python.org/3/library/re.html) library. Using `search()` we can take a certain pattern and look for it in a text. This function will return a `Match` object, from which we can obtain the text portion that was matched by our pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for \"Madrid\":\n",
      "<re.Match object; span=(7, 13), match='Madrid'>\n",
      "\n",
      "Looking for \"Rome\":\n",
      "None\n",
      "\n",
      "Looking for \"Lisbon\":\n",
      "<re.Match object; span=(0, 6), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "print(\"Looking for \\\"Madrid\\\":\")\n",
    "match = re.search(\"Madrid\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Rome\\\":\")\n",
    "match = re.search(\"Rome\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Lisbon\\\":\")\n",
    "match = re.search(\"Lisbon\", text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is already possible to observe some things:\n",
    "\n",
    "- When there is no match, `search()` returns `None`.\n",
    "\n",
    "- The `Match` object has the index of the beginning and ending of the match. Might be used via `match.start()` and `match.end()`.\n",
    "\n",
    "- If there is more than one instance of the word in the text only the first will be retrieved.\n",
    "\n",
    "If we want to return all the matches to our pattern in a given text we might use the function `findall()`. In this case, the matched portions of the text will be returned, instead of the `Match` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon\n",
      "Lisbon\n",
      "Lisbon\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, one of the words was written as _Lisbona_ , but we still match the _Lisbon_ portion of that word. If we add the condition of having a white space after the letter *n* we will only get two matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon \n",
      "Lisbon \n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\\s\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we really want the `Match` objects for some reason, `finditer()` should be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='Lisbon'>\n",
      "<re.Match object; span=(14, 20), match='Lisbon'>\n",
      "<re.Match object; span=(34, 40), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking at some of the previously shown codes at cheatsheet, let's see in some simple examples how that may help us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"x xy xyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering what we've shown previously, `.` will match any character after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x ', 'xy', 'xy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"x.\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*` will match 0 or more y symbols after xy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xy', 'xyy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy*\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`+` will match 1 or more y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xy', 'xyy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`?` will match 0 or 1 y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xy', 'xy']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy?\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{i}` will match i y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy{2}\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"lotterer Jani Senna conway Kobayashi Lopez buemi Nakajima alonso\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to match only the names that start with capital letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to match all the names that don't start with letters \"B\" and \"L\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jani', 'Senna', 'conway', 'Kobayashi', 'Nakajima', 'alonso']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\b[^bBlL\\s][A-Za-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering what that hacky `r` is doing before the actual regex we are using. This has no connection with regex. It is just a way of telling python that it should interpret backslashes `\\` literally (Notice how our regex has `\\b` and `\\s`). For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With r:\n",
      "\n",
      "lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\n",
      "\n",
      "\n",
      "Without r:\n",
      "\n",
      "lotterer \n",
      " Jani \n",
      " Senna conway Kobayashi Lopez buemi Nakajima alonso\n"
     ]
    }
   ],
   "source": [
    "print(\"With r:\\n\")\n",
    "print(r\"lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\")\n",
    "print(\"\\n\")\n",
    "print(\"Without r:\\n\")\n",
    "print(\"lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case, since we are using `r` the model takes `\\n` literally and in the second case, python interprets it as the escaped symbol for newline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Imagine now we have some extra information in front of the names and that we receive a file with many lines. We still want only the names that start with capital letters. So we run the previous regex and..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"lotterer Rebellion\\nJani Rebellion\\nSenna Rebellion\\nconway Toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima Toyota\\nalonso Toyota\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rebellion',\n",
       " 'Jani',\n",
       " 'Rebellion',\n",
       " 'Senna',\n",
       " 'Rebellion',\n",
       " 'Toyota',\n",
       " 'Kobayashi',\n",
       " 'Toyota',\n",
       " 'Lopez',\n",
       " 'Toyota',\n",
       " 'Toyota',\n",
       " 'Nakajima',\n",
       " 'Toyota',\n",
       " 'Toyota']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we don't want those extra names in there. So let's try to add the symbol `^` to make sure the expression only captures the beginning part of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum.. we got a handful of nothing. Why is this happening? Well the regex processes all the text as a single line, and the first name is not one that starts with a capital letter. To make sure this is the case, let's change `lotterer` to `Lotterer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Lotterer Rebellion\\nJani rebellion\\nSenna Rebellion\\nconway toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima toyota\\nalonso Toyota\"\n",
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still only capture one line. Luckily, we have [`re.MULTILINE`](https://docs.python.org/3/library/re.html#re.MULTILINE), that allows us to process multiline strings easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer', 'Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^[A-Z][a-z]+\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we were able to get all the information we wanted! And what if we wanted the second part of each line? Well, in this case, that is the last word of the line, so we may use `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rebellion', 'Rebellion', 'Toyota', 'Toyota', 'Toyota', 'Toyota']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+$\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can get hard to read really fast, but even knowing the basics will be certainly helpful sometime in the future. To better understand how they work nothing is better than practicing, and sites like [this](https://regexr.com/3lvai) and [this](https://regex101.com/) are valuable visuals tool to do so. The python library that we used has a lot of more powerful methods too, which might be useful to future tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important step when dealing with text data is to _tokenize_ the data. In practice what this means is splitting the strings of a corpus into substrings. This is important because it transforms a string into parts that are more suitable to be used by the tools that exist in natural language processing. For instance, if we are working with the sentence\n",
    "\n",
    "_\"The car went too fast on the second lap. This damaged the tires.\"_ ,\n",
    "\n",
    "would be better approached as a list,\n",
    "\n",
    "_[\"The\", \"car\", \"went\", \"too\", \"fast\", \"on\", \"the\", \"second\", \"lap\", \".\", \"This\", \"damaged\", \"the\", \"tires\", \".\"]_ .\n",
    "\n",
    "We will be using [NLTK](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The car went too fast on the second lap. This damaged the tires...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'This', 'damaged', 'the', 'tires', '...']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokenizer is created taking advantage of the learned regular expressions. This means that we can make different tokenizer, accordingly to what we want. For instance, if we had used `[A-Z]\\w+`, the tokenizer would only select the words that begin with capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'This']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('[A-Z]\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are already some pre-defined implementations by taking advantage of `RegexpTokenizer`. These are:\n",
    "- `BlanklineTokenizer` - Tokenize a string using blank lines as delimiter.\n",
    "- `WordPunctTokenizer` - Tokenize a string into alphabetic and non-alphabetic characters.\n",
    "- `WhitespaceTokenizer`-  Tokenize a string using spaces, tabs and newlines as delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The car went too fast on the second lap. This damaged the tires...']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlanklineTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'car',\n",
       " 'went',\n",
       " 'too',\n",
       " 'fast',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'lap',\n",
       " '.',\n",
       " 'This',\n",
       " 'damaged',\n",
       " 'the',\n",
       " 'tires',\n",
       " '...']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'car',\n",
       " 'went',\n",
       " 'too',\n",
       " 'fast',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'lap.',\n",
       " 'This',\n",
       " 'damaged',\n",
       " 'the',\n",
       " 'tires...']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhitespaceTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `WordPunctTokenizer()` is similar to the first one we defined. This is what is commonly used and the default method of tokenization that will be used when we talk about the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming allows us to get the \"root\" of words. This is important because in certain tasks we are more interested in a broader representation of a given word and not specific variation of it, like its plural, for instance. Before using the stemmer it is necessary to download some tools required by nltk, regarding the language we want to use. We will be working with the English language, using the NLTK Downloader, the same way we would import nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christinemaroti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's see what this step gets us for the same example we have been using. To do that we will be using the NLTK implementation of the [snowball stemmer](https://www.nltk.org/api/nltk.stem.html#nltk.stem.snowball.SnowballStemmer). Notice that there are other stemmers, some of them specific to certain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'this', 'damag', 'the', 'tire', '...']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "words = tokenizer.tokenize(text)\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "stems = [list(map(stemmer.stem, words))]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that _\"damage\"_ and _\"tires\"_ are transformed into simpler forms of the respective words. Notice as well that all the words have been lowercased. Lowercasing the data is also a common step in text pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that you may have noticed was the concept of \"stopwords\" being used. **Stopwords** are common words in a given corpus or language that, due to being so common, lose interest for most natural language processing applications. \n",
    "\n",
    "For instance, imagine a search engine, looking through a whole range of documents. Words as \"*the*\", \"*a*\", \"*at*\", etc. will be present in so many documents that using them in the search will not reduce the amount of possible files that could be relevant to our query. So filtering them out is beneficial to our goal.\n",
    "\n",
    "In the specific case of the stemmer function we are using, defining `ignore_stopwords` as `True` will prevent the stemming of stopwords.\n",
    "\n",
    "In the next part of this BLU you will read about stopwords again, as they are important for the task you will be doing there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides stemming there is also the process of **lemmatization**. Both processes share the goal of getting the root of the word, or more formally, reduce inflectional forms of a word to a common base form [\\[7\\]](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), but they act differently. Whereas stemming follows an heuristic approach that drops the suffix of words in order to get closer to the common base form, lemmatization uses a dictionary and morphological analysis of words to return the base form of words, known as _lemma_.\n",
    "\n",
    "Using the example in the cited reference, if shown the word _saw_ , stemming would tend to return only *s* , while lemmatization would take into account if the word was the verb or the noun, and correspondingly, return _see_ or _saw_  as the base form of the word.\n",
    "\n",
    "As you may expect, lemmatization is much more expensive in computational terms and, for certain applications, stemming might be more than enough to obtain good results. We will be using only stemming throughout the NLP learning units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_n-grams_ correspond to sequences of n consecutive elements from a given sentence. Commonly each element is a word, or \"token,\" but we may define it as we wish for the task at hand. Usually we refer to unigrams, bigrams, trigrams, four-grams, etc. according to the length of the sequence of elements.\n",
    "\n",
    "For instance, for the sentence\n",
    "\n",
    "`\"The driver made a mistake\"`,\n",
    "\n",
    "we would have:\n",
    "\n",
    "- unigrams: `The`, `driver`, `made`, `a`, `mistake`\n",
    "- bigrams: `The driver`, `driver made`, `made a`, `a mistake`\n",
    "- trigrams: `The driver made`, `driver made a`, `made a mistake`\n",
    "- four-grams: `The driver made a`, `driver made a mistake`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create _n-grams_ but taking advantage of the [NLTK ngram](http://www.nltk.org/_modules/nltk/model/ngram.html) implementation. We will be using the tokenized list `words` created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'This', 'damaged', 'the', 'tires', '...']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The',), ('car',), ('went',), ('too',), ('fast',), ('on',), ('the',), ('second',), ('lap',), ('.',), ('This',), ('damaged',), ('the',), ('tires',), ('...',)]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'car'), ('car', 'went'), ('went', 'too'), ('too', 'fast'), ('fast', 'on'), ('on', 'the'), ('the', 'second'), ('second', 'lap'), ('lap', '.'), ('.', 'This'), ('This', 'damaged'), ('damaged', 'the'), ('the', 'tires'), ('tires', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'car', 'went'), ('car', 'went', 'too'), ('went', 'too', 'fast'), ('too', 'fast', 'on'), ('fast', 'on', 'the'), ('on', 'the', 'second'), ('the', 'second', 'lap'), ('second', 'lap', '.'), ('lap', '.', 'This'), ('.', 'This', 'damaged'), ('This', 'damaged', 'the'), ('damaged', 'the', 'tires'), ('the', 'tires', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by looking at the output, it's possible to observe that we are getting what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams may be used for several things, like extra features in natural language processing classification tasks. Imagine counts of \"very good\" vs. \"very\" and \"good\" individually when doing sentiment analysis, or the difference in the counts of n-grams present in a refence and our hypothesis as a way of calculating similarity between generated texts, and so on. Another interesting use-case is **n-gram language models**. This kind of model predicts the next item (in this case a word) in a given sequence based on *n* past items, using conditional probabilities.\n",
    "\n",
    "More formally, following the notation in reference [\\[6\\]](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf), the probability of a given sentence will be given by\n",
    "\n",
    "$$ P(w_1w_2...w_n) = \\prod_i P(w_i \\lvert w_1w_2...w_{i-1})$$\n",
    "\n",
    "But a conditional probability on all the past context doesn't seem very wise. Therefore we will make use of the Markov property, and assume that each of the right hand side components of the product is conditioned on the _n-1_ past most recent elements.\n",
    "\n",
    "$$ P(w_1w_2...w_n) = \\prod_i P(w_i \\lvert w_{i-n}...w_{i-1})$$\n",
    "\n",
    "Estimating the probabilities of the _n-grams_ is a matter of taking their counts in a given training corpus and calculating the probabilities.\n",
    "\n",
    "$$ P(w_i \\lvert w_{i-n}...w_{i-1}) = \\frac{count(w_{i-n},...,w_{i-1},w_i)}{count(w_{i-n},...,w_{i-1})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a simple example, forgetting n-grams of words for a while, imagine that you have three possible states for the weather: _rainy_ , _sunny_ and _cloudy_ . Having counts for each of the occurrences, for instance (rainy, rainy), (rainy, cloudy), (rainy, sunny), (rainy, rainy, sunny), etc. you may calculate the probabilities for each of the transitions. Therefore, you may be able calculate the probability of a given sequence of weather occurrences or even guess what is the most probable weather condition for the following day. If you're curious, use [this page](http://setosa.io/ev/markov-chains/) to get a more interactive approach to a similar example.\n",
    "\n",
    "Going back to n-grams of words, you just have to replace weather states by words and you are able to find the most probable sentences or even generate text by choosing the most probable word given its context. For the latter you would be producing a very simple language model (a computational model that is able to output text that should be somewhat fluent). If you're already familiar with some of these things and want to take it up a notch regarding n-gram language models go through [this set of slides](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf) that cover some practical considerations to take into account when building such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word of Advice\n",
    "\n",
    "Even though we are using NLTK library during this BLU, there are some other libraries that are commonly used and probably better. Here is a list of some to consider in your future challenges in NLP:\n",
    "\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/other-languages.html#python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] - [RegExr](https://regexr.com/3lvai)\n",
    "\n",
    "\\[2\\] - https://pymotw.com/2/re/\n",
    "\n",
    "\\[3\\] - [NLTK Book](https://www.nltk.org/book/)\n",
    "\n",
    "\\[4\\] - [N-grams](https://en.wikipedia.org/wiki/N-gram#n-gram_models)\n",
    "\n",
    "\\[5\\] - [Language Model](https://en.wikipedia.org/wiki/Language_model)\n",
    "\n",
    "\\[6\\] - [Introduction to N-grams](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf)\n",
    "\n",
    "\\[7\\] - [Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Regex: [RegexOne](https://regexone.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
