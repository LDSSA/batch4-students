{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/imdb_sentiment.csv')\n",
    "\n",
    "# Get the text\n",
    "docs = df['text']\n",
    "\n",
    "# Split in train and validation\n",
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all of this in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part I of this BLU you learned about _preprocessing_ your text data into something easier for the machine later to process and vectorize. In practice, we can create a class that transforms our data into those _easier to process_ strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        \n",
    "        # Replace given regexes\n",
    "        for regex in self.regex_list:\n",
    "            sentence = re.sub(regex[0], regex[1], sentence)\n",
    "            \n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a class that has a `transform()` method that will apply the method `_clean_sentence()` to every sentence of its input `X`. Note that you can choose the tokenizer and the stemmer as inputs of this class - you can choose which ones you prefer to use. You can also give the class a list of tuples that are regexes that you want to substitute for something in your sentences.\n",
    "\n",
    "Let's use the same tokenizer, stemmer and html regex that we were using before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer and a stemmer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "regex_list = [(\"<[^>]*>\", \"\")\n",
    "             ]\n",
    "\n",
    "cleaner = TextCleanerTransformer(tokenizer, stemmer, regex_list)\n",
    "docs = cleaner.transform(train_df.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an output example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now onto what we learned in Part II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, there are many implementations in the internet of Bag of Words and TF-IDF. We're going to use scikit-learn from now on to compute the feature representations learnt in this BLU.\n",
    "\n",
    "Our BoW representations, for instance, can be done with scikit's [CountVectorizer()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). We still haven't removed stopwords, as you may have noticed. However, removing stopwords is easy with `CountVectorizer()` - just pass the parameter `stop_words` and assign it to `'english'`! This will remove all english stopwords from your corpus. You can also give this parameter a list of strings, if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(docs)\n",
    "\n",
    "# Looking at a small sample of the vocabulary:\n",
    "vocabulary = list(vectorizer.vocabulary_.keys())\n",
    "print(\"Small sample of the vocabulary:\", vocabulary[0:20])\n",
    "\n",
    "# Number of words in the vocabulary\n",
    "print(\"\\nNumber of distinct words:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a random sample sentence, for example sentence 12, we can visualize our bag of words representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = docs[12:13]\n",
    "print(sentence[0], '\\n')\n",
    "\n",
    "# Tranform sentence into bag of words representation\n",
    "word_count_sentence = vectorizer.transform(sentence)\n",
    "\n",
    "# Find the indexes of the words which appear in the sentence\n",
    "_, columns = word_count_sentence.nonzero()\n",
    "\n",
    "# Get the inverse map to map vector indexes to words\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "inv_map = {v: k for k, v in vocabulary.items()}\n",
    "\n",
    "# Extract the corresponding word and count\n",
    "counts = [(inv_map[i], word_count_sentence[0, i]) for i in columns]\n",
    "\n",
    "for word, count in counts:\n",
    "    print(word, \": \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the word counts (Bag of Words representation) for every sentence by calling the transform method. This returns a sparse matrix where the rows represent the samples and the columns the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix = vectorizer.transform(df['text'].values)\n",
    "word_count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do TF-IDF, that can be done on top of our matrix of word counts with [TfidfTransformer()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(word_count_matrix)\n",
    "\n",
    "word_term_frequency_matrix = tfidf.transform(word_count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in Part I, we can not only get features that correspond to each word in our vocabulary (Unigram) but also can get features for any number of N-grams. We can easily get all the possibilities in a chosen range of N by using `CountVectorizer()` parameter `ngram_range`. This parameter receives a tuple `(min_n, max_n)` - if we wanted to extract all unigrams and bigrams in a corpus we would pass to the model `ngram_range=(1,2)`.\n",
    "\n",
    "Let's get all unigram, bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_123_grams = CountVectorizer(stop_words= 'english', ngram_range=(1,3))\n",
    "vectorizer_123_grams.fit(docs)\n",
    "word_count_matrix = vectorizer_123_grams.transform(df['text'].values)\n",
    "word_count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we end up with a much bigger feature space - which can be quite computationally expensive to our model! Fortunately, there are some parameters in `CountVectorizer()` that we can use to reduce our feature space while keeping as much informative representations as possible:\n",
    "\n",
    "- `max_features` - receives an `int` that will be the size of the feature space of the model. The N features chosen will be the ones with higher term frequency across the corpus.\n",
    "- `min_df` - the minimum document frequency a n-gram can have to be considered. Often called *cut-off*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the sentiment of the movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use all that we learned and create a model that predicts if a review is positive or negative. In NLP, we call this task _sentiment analysis_.\n",
    "\n",
    "<img src=\"./media/dwight.jpg\" width=\"500\">\n",
    "\n",
    "Since there are several things that we need to do sequentially to our data, we can create a [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Pipelines allow us to easily compose transformations and classifiers.\n",
    "\n",
    "The main advantage of pipelines is that the pipeline exposes the fit and predict functions, these automatically call the transformations on the data and the classifier, keeping the transformations coherent between train and test data.\n",
    "\n",
    "We will use Scikit's implementation of Naive Bayes as our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english')),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece that is missing is converting the character labels into numeric labels through the Scikit [Label Encoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'].values)\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'].values)\n",
    "validation_df['sentiment'] = le.transform(validation_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use bigrams as well in our model, along with unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, our performance on the validation set got worse! This is an example of when it can be hurtful to your model to remove stopwords, as we warned you in Part II. If you take a look at stopwords list, words like \"no\" are part of it. This can be crucial to our bigram representation since if we remove these words, relevant bigrams will not appear in our feature space (ex.: \"no fun\").\n",
    "\n",
    "Let's remove the stop_words parameter from the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a better performance, but still not as good as in our unigram model. This can be due to our feature space having too many dimensions, which can hurt model performance. Fortunately, we learned that `CountVectorizer()` has parameters that help to reduce our feature dimensionality.\n",
    "\n",
    "You should try and see if you can get higher accuracy by reducing dimensionality with `max_features` and `min_df` (Spoiler alert: you will!). You can then also try to use trigrams along with the unigrams and bigrams you already have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out different classifiers and play around with `CountVectorizer()` and `TfidfTransformer()` parameters to see if you can get better scores!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
