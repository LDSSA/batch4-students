{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - SVD and PCA\n",
    "\n",
    "In the previous part we introduced you to dimensionality reduction and we showed you how you can do it with simple heuristics, feature engineering, and even with statistical methods. But, as mentioned, these methods lack information such as interaction between features and can be over-simplified for our purposes. This is where algebra kicks in to help us out.\n",
    "\n",
    "![morpheus-algebra](./media/what-if-i-told-you.jpg)\n",
    "\n",
    "\n",
    "Never thought you'd use algebra again, right? Guess again, Neo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure,cla,close\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Singular Value Decomposition\n",
    "\n",
    "The Singular Value Decomposition (SVD) method is a matrix decomposition or matrix factorization method, often used in applications such as compression, denoising and, our current topic, dimensionality reduction. All matrices have an SVD, making it more stable than other methods, such as the eigendecomposition. It is often presented with Principal Component Analysis (PCA), which we are going to dive into later, but it can actually be used by itself, so let's start with that.\n",
    "\n",
    "The concept of SVD is relatively straightforward, but it is useful to understand the algebra behind it. I was actually going to assume you are fresh on your basic linear algebra (don't roll your eyes at me), skip the primers and jump right to business, but then again, a little refresher doesn't hurt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear Algebra\n",
    "\n",
    "<img src=\"./media/welcome-to-the-matrix.jpg\" width=\"400\">\n",
    "\n",
    "A matrix is just a collection of values ordered in rows and collumns, such as the one that you created with count vectorizer or Tf-idf vectorizer.\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "    x_{11}&x_{12}&.&.&.&.&x_{1N}\\\\\n",
    "    x_{21}&x_{22}&.&.&.&.&x_{2N}\\\\\n",
    "    .&.&.&.&.&.&.\\\\\n",
    "    x_{M1}&x_{m2}&.&.&.&.&x_{MN}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It has a known shape - in our case above M-by-N. Remember that a 1-by-N or M-by-1 shaped matrix is what we normally call a vector. There are also some typical operations that you compute usually, in particular with the help of numpy, like the transpose:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "    a & b & c\\\\\n",
    "    d & e & f \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad A^T = \\begin{bmatrix} \n",
    "    a & d \\\\\n",
    "    b & e \\\\\n",
    "    c & f \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or the dot product, where the internal dimensions have to match:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "    a & b\\\\\n",
    "\\end{bmatrix} \n",
    "\\quad B = \\begin{bmatrix} \n",
    "    c \\\\ \n",
    "    d \\\\\n",
    "\\end{bmatrix} \n",
    "\\quad AB = \\begin{bmatrix} \n",
    "    ac + bd \n",
    "\\end{bmatrix} \n",
    "\\quad A^T B^T = \\begin{bmatrix} \n",
    "    ac & ad \\\\ \n",
    "    bc & bd \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "An important concept is also the determinant of a matrix, an operation applicable to squared matrix that yields a scalar value, that can be seen geometrically as the scaling factor of the linear transformation described by the matrix. For simple matrices, like 2-by-2 and 3-by-3, the expressions seem fairly simple:\n",
    "\n",
    "$$ A = \\begin{bmatrix} \n",
    "    a & b \\\\\n",
    "    c & d \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad|A| = ad − bc \n",
    "\\quad \\quad B = \\begin{bmatrix} \n",
    "    a & b & c\\\\\n",
    "    d & e & f \\\\\n",
    "    g & h & i \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad|B| = a(ei − fh) - b(di − fg) + c(dh − eg)\n",
    "$$\n",
    "\n",
    "But for matrices with higher numbers of rows or columns, these formulas will get complicated. For all these operations, if you wish to understand more about linear algebra, you'll have to study a bit. But for now, this should suffice to grasp the mathematics behind the SVD. First, try out a few of these operations (in particular the determinant) with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [1, 2],\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [1, 2],\n",
    "])\n",
    "\n",
    "# Print transpose\n",
    "A_T = A.T\n",
    "print(A_T)\n",
    "\n",
    "# Use matrix multiplication\n",
    "# np.multiply is the element-wise multiplication, while np.dot or np.matmul are matrix multiplications\n",
    "A_mul = np.matmul(A, A_T)\n",
    "\n",
    "# Compute determinant of 6x6 matrix\n",
    "np.linalg.det(A_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it. If we consider a linear transformation as being represented by some matrix $M$, then what this mean is that the product of this matrix by the eigenvector $v$ will just be a scaled version of $v$:\n",
    "\n",
    "$$Mv=\\lambda v$$\n",
    "\n",
    "In this definition we exclude the zero-vector, since the multiplication would always be true for any scalar $ \\lambda $. A method for finding the vectors $v$ - the **eigenvectors** - and their scalar counterparts $\\lambda$ - the **eigenvalues** - is to treat the above as a linear system, and solve it. We are not going to do a demonstration on this method because the most common way of solving this is actually through algebra, using the determinant. We will not go deeper into how to compute this, since numpy makes it super easy, but you should at least understand its reasoning and know the basic equation that defines them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues and vectors of square matrix\n",
    "lmbU, U = np.linalg.eig(A_mul)\n",
    "\n",
    "print(lmbU)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Defining the Singular Value Decomposition (SVD)\n",
    "\n",
    "The SVD of a matrix $A$ is just its factorization into the product of three matrices, $U$, $S$, and $V$ where the columns of U and V are orthonormal and $S$ is diagonal with positive real entries, described by the following:\n",
    "\n",
    "$$ A = USV^T $$\n",
    " \n",
    "* $A$: matrix we want to decompose (consider $A$ an M-by-N matrix)\n",
    "* $U$: orthonormal$^1$ matrix composed by eigenvectors of  $AA^T$. These eigenvectors are also called the 'left-singular' values of A. ($U$ is M-by-M)\n",
    "* $V^T$: orthonormal$^1$ matrix composed by eigenvectors of $A^TA$. These eigenvectors are also called the 'right-singular' values of A. ($V$ is N-by-N)\n",
    "* $S$: diagonal matrix containing the square root of nonzero eigenvalues of $U$ (or $V$), ordered by decreasing size ($S$ is M-by-N). It is a property of these matrices that the nonzero eigenvalues of $U$ and $V$ are the same, this is, the eigenvectors of  $AA^T$ and $A^TA$ are related.\n",
    "\n",
    "$^1$ Orthonormal matrices have the following property: $VV^T = I$. This is, their product by its transpose produces the identity matrix.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's try to perform a decomposition then. First, let's define a matrix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Choose A\n",
    "\n",
    "Let's create a very simple matrix  A imagining we have a bunch of reviews that we classify as positive or negative from the words that appear there.\n",
    "\n",
    "|          |  like | hate | not | love |\n",
    "|----------|-------|------|-----|------|\n",
    "| Positive |  5    |  0   |  2  |   4  |\n",
    "| Negative |  2    |  6\t  |  1  |   2  |\n",
    "\n",
    "\n",
    "You can see how this matrix can grow with the number of examples, and the number of n-grams considered, which will typically lead to a higher vocabulary. Thus, you can understand why it is desirable to simplify it.\n",
    "\n",
    "Start by defining the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [5, 0, 2, 4],\n",
    "    [2, 6, 1, 2]\n",
    "])\n",
    "\n",
    "print(\"Matrix to decompose:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create U, V and S from what we learned above:\n",
    "\n",
    "### 2 - Build U \n",
    "\n",
    "Remember that $U$ is composed by the eigenvectors of  $AA^T$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT = A.T\n",
    "AAT = np.matmul(A, AT)\n",
    "\n",
    "lmbU, U = np.linalg.eig(AAT)\n",
    "\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(U)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(lmbU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Build $V^T$\n",
    "\n",
    "Remember that $V^T$ is composed by the eigenvectors of  $A^TA$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATA = np.matmul(AT, A)\n",
    "\n",
    "lmbV, V = np.linalg.eig(ATA)\n",
    "\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(V)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(lmbV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that these are basically the same eigenvalues that of $U$, padded with near-zero values at the end. We are only missing $S$ now, so let's build it. \n",
    "\n",
    "### 4 - Create S\n",
    "\n",
    "Take the square-roots of the nonzero eigenvalues and place them on the diagonal of a matrix size M-by-N, padding with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_shape = A.shape\n",
    "\n",
    "# Pick which lmb to use (higher rank)\n",
    "lmb = lmbV if lmbV.shape[0] > lmbU.shape[0] else lmbU\n",
    "\n",
    "# Cut off to the correct dimensions\n",
    "S = np.sqrt(np.diag(abs(lmb))[:S_shape[0],:S_shape[1]])\n",
    "\n",
    "print(\"S matrix:\")\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check that the factorization was successful and assure that $A = USV^T$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original matrix:\")\n",
    "print(A)\n",
    "\n",
    "USVT = np.matmul(np.matmul(U, S), V.T)\n",
    "\n",
    "print(\"\\nFactorization U S V^T\")\n",
    "print(USVT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we got to the same values of the initial matrix, scaled by a factor of -1. This is an ambiguity of the SVD problem, but there are ways to solve it. However, for our problem, you could simply consider that, knowing we are dealing with counts (if we are using CountVectorizer) or just a scaled version of it (TfIdfVectorizer), the values must be positive, and we could simply use that information to always disambiguate the sign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = USVT.sum()/abs(USVT.sum())\n",
    "\n",
    "print(\"Matrix sign:\")\n",
    "print(sign)\n",
    "\n",
    "USVT_disambiguated = sign * USVT\n",
    "print(\"\\nFactorization U S V^T disambiguated:\")\n",
    "print(USVT_disambiguated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this method to disambiguate is only valuable inside our context, but there are other ways of solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still want a deeper mathematical explanation you can start with this __[chapter](http://www.deeplearningbook.org/contents/linear_algebra.html)__, or if you've completely forgotten linear algebra and want to relearn it you can also try this __[course](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm)__.\n",
    "\n",
    "![neo-algebra](./media/i-know-linear-algebra.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVD for dimensionality reduction\n",
    "\n",
    "Now that you know what's under the hood for SVD you probably want to make some use of this, so we will show you how to use it to perform feature reduction. It is actually quite simple. Let's take a step back. Because of the orthonormal property of our matrix $V$ we can write the SVD equation as:\n",
    "\n",
    "$$ AV=US $$\n",
    "\n",
    "We could now assume a truncation of k components (let's say, to 1 component) and get the following:\n",
    "\n",
    "$$ A\\begin{bmatrix}v_{11}\\\\v_{21}\\\\.\\\\.\\\\v_{n1}\\end{bmatrix} = s_{11}\\begin{bmatrix}u_{11}\\\\u_{21}\\\\.\\\\.\\\\u_{m1}\\end{bmatrix} $$\n",
    "\n",
    "The vector $v$ is what we call a projection direction. The result of multiplying $A$ by this direction will result in a matrix with the same number of rows, but a smaller number of columns. Your data would now be projected in the first direction, this is, the one holding more variance. We could then get an approximation of $A$ by computing:\n",
    "\n",
    "$$ A \\approx s_{11}\\begin{bmatrix}u_{11}\\\\u_{21}\\\\.\\\\.\\\\u_{m1}\\end{bmatrix}\\begin{bmatrix}v_{11}&v_{21}&.&.&v_{n1}\\end{bmatrix} $$\n",
    "\n",
    "So you could now extend this for k components. By selecting the first k eigenvalues in S and assuming the others to be zero, you can use any subselection to reconstruct an approximate version of  A, with:\n",
    "\n",
    "$$ A^{*}=U^{*}S^{*}{V^{*}}^{T} $$\n",
    "\n",
    "Where $S^{*}$ is reduced to keep the first k eigenvalues and both $U^{*}$ and ${V^{*}}^{T}$ are reduced such that the dimensions are consistent. But this reconstruction isn't actually what we want. What we want is the projection of our data into a smaller space. In the same way that with 1 component multiplying by $V$ gave us the projected data, the same holds for k components, and all we need to do is project given $V$:\n",
    "\n",
    "\n",
    "$$ A_r = A {V^{*}} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "By selecting the first k eigenvalues we ensure that  $A^{*}$ retains a disproportionately high amount of the variance of  A; we have in effect compressed the original information and represented it using fewer features. This is often used in NLP and known in that context as LSA - Latent Semantic Analysis\n",
    "\n",
    "\n",
    "##  1.2 ) Putting it all together\n",
    "\n",
    "Let's put together all the steps we've seen and try out dimensionality reduction with SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svd_decomposition(A):\n",
    "    AT = A.T\n",
    "    AAT = np.matmul(A, AT)\n",
    "    ATA = np.matmul(AT, A)\n",
    "\n",
    "    lmbU, U = np.linalg.eig(AAT)\n",
    "    lmbV, V = np.linalg.eig(ATA)\n",
    "\n",
    "    S_shape = A.shape\n",
    "    lmb = lmbV if lmbV.shape[0] > lmbU.shape[0] else lmbU\n",
    "    S = np.sqrt(np.diag(abs(lmb))[:S_shape[0],:S_shape[1]])\n",
    "    \n",
    "    return U, S, V\n",
    "\n",
    "\n",
    "def get_svd_reduction_counts(A, n_components, debug=False):\n",
    "    \n",
    "    U, S, V = get_svd_decomposition(A)\n",
    "    \n",
    "    USVT = np.matmul(np.matmul(U[:, :n_components], S[:n_components, :n_components]), V.T[:n_components,:])\n",
    "    \n",
    "    # Assuming our initial matrix A holds counts or scaled counts\n",
    "    sign = USVT.sum()/abs(USVT.sum())\n",
    "    USVT_disambiguated = sign * USVT\n",
    "\n",
    "    if debug:\n",
    "        print(\"Reconstruction from {} components\".format(n_components))\n",
    "        print(USVT_disambiguated)\n",
    "\n",
    "    reduction = np.matmul(A, V[:,:n_components])\n",
    "    return reduction, V.T[:n_components,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's start with a simple A, from a bunch of reviews. We will now add two extra features that we'll define in an agnostic way regarding the positive/negative reviews:\n",
    "\n",
    "|          |  like | hate | not | love | movie | book |\n",
    "|----------|-------|------|-----|------|-------|------|\n",
    "| Positive |  5    |  0   |  2  |   4  |   1   |   0  |\n",
    "| Negative |  2    |  6\t  |  1  |   2  |   0   |   1  |  \n",
    "| Positive |  3    |  1   |  1  |   4  |   2   |   1  |\n",
    "| Negative |  2    |  6\t  |  1  |   5  |   0   |   2  |\n",
    "| Negative |  0    |  0   |  3  |   1  |   0   |   0  |\n",
    "| Negative |  2    |  4\t  |  0  |   1  |   1   |   0  |\n",
    "\n",
    "Let's define our matrix as a numpy array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [5, 0, 2, 4, 1, 0],\n",
    "    [2, 6, 1, 2, 0, 1],\n",
    "    [3, 1, 1, 4, 2, 2],\n",
    "    [2, 6, 1, 5, 0, 2],\n",
    "    [0, 0, 3, 1, 0, 0],\n",
    "    [2, 4, 0, 1, 1, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our functions to get a reduction and compute the variance each reduction holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = []\n",
    "\n",
    "for n in range(6):\n",
    "    A_red, VT = get_svd_reduction_counts(A, n_components=n+1)\n",
    "    variance.append(1.0*np.var(A_red, axis=0).sum() / np.var(A, axis=0).sum())\n",
    "\n",
    "plt.plot(np.array([1.0*(n+1) for n in range(6)]), np.array(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the variance increases up until 4 features and then stabilizes. However, be mindful that the number of components is not directly related to the components in our table. We did add two components at the end that we know shouldn't have a huge impact, and this results seems to make sense in the sense that 4 components computed from the SVD would be enough to cover almost the entire set's variance.\n",
    "\n",
    "But if you reorder the columns or if you have further relations between your features you could even see 2 components holding the entire variance. See what happens when we copy the values in \"like\" to \"love\" and \"hate\" to \"not\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [5, 0, 0, 5, 1, 0],\n",
    "    [2, 6, 6, 2, 0, 1],\n",
    "    [3, 1, 1, 3, 2, 2],\n",
    "    [2, 6, 6, 2, 0, 2],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [2, 4, 4, 2, 1, 0],\n",
    "])\n",
    "\n",
    "variance = []\n",
    "\n",
    "for n in range(6):\n",
    "    A_red, VT = get_svd_reduction_counts(A, n_components=n+1)\n",
    "    variance.append(1.0*np.var(A_red, axis=0).sum() / np.var(A, axis=0).sum())\n",
    "\n",
    "plt.plot(np.array([1.0*(n+1) for n in range(6)]), np.array(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you can in fact attribute most of the variance to only two components, at about 95%. As a rule of thumb, you should aim at keeping over 85% of the variance explained. So our 95% result is actually very good, and it should show you that the SVD actually extracts meaningful features from your data. Feeling pretty awesome there, Neo?\n",
    "\n",
    "<img src=\"./media/neo-dodging-bullets.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "Unfortunately not all cases - in particular, real cases - are so straightforward. Instead of using our implementation, we will move to the scikitlearn version, where you can use the TruncatedSVD class to apply this technique to your data, with more additional options. We will try it out with the data from the previous example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  SVD - NLP practical example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Start from baseline\n",
    "\n",
    "As before, start by importing the dataset and getting a baseline. We will change from the multinomial naive Bayes we were using to a classifier that can handle negative numbers, because of the output of SVD. For now let us use a KNeighborsClassifier, which will try to classify the test points by their proximity to training examples. This is a very good classifier with which to make a case for dimensionality reduction, since in a very high-dimensional space, the vector distances will most of the time be useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "df = pd.read_csv('./datasets/twitter_rep_dem_data_small.csv')\n",
    "\n",
    "# Feature Extraction\n",
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "train_data, test_data = train_test_split(df,  test_size=0.3, random_state=seed)\n",
    "      \n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "y_train = train_data.Party\n",
    "y_test = test_data.Party\n",
    "\n",
    "# Baseline\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty low baseline, as expected. It is actually similar to just randomly attribute classes to our examples. Let's see if keeping the same algorithm we can get to some decent values by reducing the dimensionality of the data.\n",
    "\n",
    "## 2.2 - Applying SVD\n",
    "\n",
    "We will now use the scikit-learn truncatedSVD class to obtain an SVD reduction of our data. This step might take a few minutes (~10 min), so just go ahead and grab a coffee, think about the nature of your reality, maybe try to bend some spoons, whatever makes you happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_variance = []\n",
    "explained_variance = []\n",
    "accuracies = []\n",
    "\n",
    "dense_X_train = X_train.toarray()\n",
    "data_var = np.var(dense_X_train, axis=0).sum()\n",
    "\n",
    "print('Total variance:')\n",
    "print(data_var)\n",
    "\n",
    "for n in [2, 10, 100, 200, 500, 1000]:\n",
    "    print(\"\\nComputing SVD for {} components\".format(str(n)))\n",
    "    svd = TruncatedSVD(n_components=n, random_state=seed)\n",
    "    %timeit svd.fit(X_train)\n",
    "    X_train_svd = svd.transform(X_train)\n",
    "    X_test_svd =  svd.transform(X_test)\n",
    "    print('\\nVariance:')\n",
    "    print(np.var(X_train_svd, axis=0).sum())\n",
    "    true_variance.append(1.0*np.var(X_train_svd, axis=0).sum() / data_var)\n",
    "    explained_variance.append(svd.explained_variance_)\n",
    "    \n",
    "    clf =  KNeighborsClassifier()\n",
    "    clf.fit(X_train_svd, y_train)\n",
    "    y_pred = clf.predict(X_test_svd)\n",
    "    \n",
    "    accuracies.append(accuracy_score(y_pred, y_test))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([2.0, 10.0, 100.0, 200.0, 500.0, 1000.0]), np.array(true_variance))\n",
    "plt.title(\"SVD Variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([2.0, 10.0, 100.0, 200.0, 500.0, 1000.0]), np.array(accuracies))\n",
    "plt.title(\"SVD Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results should show you that high dimensionality can sometimes reduce your performance. In this particular case, neither a high number of features nor a too low number of features result in good performance. Let's move forward to our second method, which is highly related to the SVD - principal component analysis, and see if the results hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis\n",
    "\n",
    "\n",
    "This idea of applying Principal Component Analysis for dimensionality reduction is very similar to the one of SVD. In fact, SVD can be used just as a mathematical way of solving PCA and several numerical software libraries actually use SVD under the hood for their PCA routines - take as example the scikit-learn implementation. So what is PCA?\n",
    "\n",
    "In the same way as before, PCA aims to project the data into a lower dimensionality space, maximizing the variance of the projection along each component while minimizing the reconstruction error. We need to introduce a new concept - the covariance matrix and the correlation matrix.\n",
    "\n",
    "Given our training set as a sequence of data points $\\{x^1, x^2, ..., x^K\\}$, a matrix $A$ that represents this dataset, and its mean as $\\mu = \\langle A \\rangle$, the covariance matrix is represented by:\n",
    "\n",
    "$$cov(A) = \\langle(A - \\mu)(A - \\mu)^T\\rangle = \\frac{(A - \\mu)(A - \\mu)^T}{n-1}$$\n",
    "\n",
    "where each row is a variable and each column an observation, and the correlation matrix is simply\n",
    "\n",
    "$$corr(A) = \\langle A A^T\\rangle$$\n",
    "\n",
    "In the study of principal components, we are interested in the variation of the data about the mean, so we normally first compute the centered data:\n",
    "\n",
    "$$ \\bar{A} = A - \\mu$$\n",
    "\n",
    "with the result that $cov(\\bar{A}) = corr(\\bar{A})$, since the data will have zero mean.\n",
    "\n",
    "\n",
    "## 3.1) Getting the first principal component\n",
    "\n",
    "We can define now the first component as projection of our data in some direction, writing it in function of some vector $\\textbf{w}$, where the scale doesn't matter since it does not affect the information contained in this component:\n",
    "\n",
    "$$p_1 = \\textbf{w}^T \\bar{A}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "-------\n",
    "\n",
    "(If you still feel you need to review the basics of linear algebra, skip the next few equations from now and read directly the TL,DR)\n",
    "\n",
    "We choose this vector to maximize the variance of $p_1$:\n",
    "\n",
    "$$\\sigma^2_{p_1} = \\langle p_1^2 \\rangle = \\langle (\\textbf{w}^T \\bar{A})(\\bar{A}^T\\textbf{w}) \\rangle = \\textbf{w}^T \\langle \\bar{A} \\bar{A}^T\\rangle \\textbf{w} = \\textbf{w}^T cov(A) \\textbf{w} $$\n",
    "\n",
    "If we define our vector $\\textbf{w}$ as a linear combination of the eigenvectors of our covariance matrix, with weights $w_i$ an eigenvectors $\\textbf{v}_i$:\n",
    "\n",
    "$$\\textbf{w} = \\sum_i{w_i \\textbf{v}_i}$$\n",
    "\n",
    "$$\\sigma^2_{p_1} = \\textbf{w}^T cov(A) (\\sum_i{w_i \\textbf{v}_i}) = \\sum_i{w_i \\textbf{w}^T cov(A)\\textbf{v}_i}$$\n",
    "\n",
    "Remember that one of the main definitions of eigenvectors is that their direction remains unchanged when applying the linear transformation ($Mv=\\lambda v$). Also note that, if $\\textbf{v}_i$ are vectors from an orthonormal basis, then $\\textbf{v}_i\\textbf{v}_j = 0$ and $\\textbf{v}_i\\textbf{v}_i = 1$, so we can write:\n",
    "\n",
    "$$\\textbf{w}^T \\textbf{v}_i = w_i $$\n",
    "\n",
    "And our previous result simplifies to:\n",
    "\n",
    "$$\\sigma^2_{p_1} = \\sum_i{w_i \\lambda_i \\textbf{w}^T \\textbf{v}_i} = \\sum_i{w_i^2 \\lambda_i}  $$\n",
    "\n",
    "This is the quantity we want to maximize, constrained to $\\sum_i{w_i^2} = 1 $, so it should be clear that the maximum should be attained by setting $w_i = 1$ for the highest eigenvalue of $cov(A)$ and zero to all other weights. The first component will then be:\n",
    "\n",
    "$$ p_1 = \\textbf{v}_1 \\bar{A}$$\n",
    "\n",
    "And a reconstruction of $\\bar{A}$ would be obtained by:\n",
    "\n",
    "$$ \\bar{A} =  p_1 \\textbf{v}_1$$\n",
    "\n",
    "And the variance of this compoment will just be $\\sigma^2_{p_1} = \\lambda_i$ for the highest eigenvalue.\n",
    "\n",
    "-------\n",
    "\n",
    "<br>\n",
    "\n",
    "### **TL,DR**: \n",
    "You can write out the first component as a function of the eigenvectors and eigenvalues of the covariance matrix of $\\bar{A}$, if these are ordered from highest eigenvalue to lowest: \n",
    "\n",
    "$$p_1 = \\textbf{v}_1 \\bar{A}$$\n",
    "\n",
    "and the variance associated to this component will be:\n",
    "\n",
    "$$\\sigma^2_{p_1} = \\lambda_1$$\n",
    "\n",
    "This can be extended to the following components, and so the procedure to get a PCA reduction is quite simple. Let's try it out like we did for SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Choose A\n",
    "\n",
    "Lets reuse the same matrix  A that we had before:\n",
    "\n",
    "|          |  like | hate | not | love |\n",
    "|----------|-------|------|-----|------|\n",
    "| Positive |  5    |  0   |  2  |   4  |\n",
    "| Negative |  2    |  6\t  |  1  |   2  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [5, 0, 2, 4],\n",
    "    [2, 6, 1, 2]\n",
    "])\n",
    "\n",
    "print(\"\\nMatrix to decompose:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Center A and compute the covariance matrix\n",
    "\n",
    "We now need to center our data. First, be careful with the dimension you choose - you want to average across examples and get a vector with the size of your feature space. Then we can compute the covariance matrix: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.mean(A, axis=0)\n",
    "\n",
    "centA = A - u\n",
    "\n",
    "print(\"\\nCentered data:\")\n",
    "print(centA)\n",
    "\n",
    "# The covariance is an average of (A - u)(A -u)^T \n",
    "covA = np.matmul(centA.T, centA)/A.shape[0]\n",
    "\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(covA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Get the covariance matrix eigenvalues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbV, V = np.linalg.eig(covA)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Project data in first component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_red = np.matmul(centA, V[:, 0])\n",
    "\n",
    "print(A_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ) Putting it all together\n",
    "\n",
    "Let's put these steps together and try out dimensionality reduction with PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_reduction(A, n_components):\n",
    "    \n",
    "    u = np.mean(A, axis=0)\n",
    "    covA = np.matmul(A.T, A)\n",
    "    lmbV, V = np.linalg.eig(covA)\n",
    "    V_red = V[:,:n_components]\n",
    "    A_red = np.matmul(A, V_red)\n",
    "    var = lmbV\n",
    "\n",
    "    return A_red, V_red, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we are going to reuse the matrix from the SVD example:\n",
    "\n",
    "|          |  like | hate | not | love | movie | book |\n",
    "|----------|-------|------|-----|------|-------|------|\n",
    "| Positive |  5    |  0   |  2  |   4  |   1   |   0  |\n",
    "| Negative |  2    |  6\t  |  1  |   2  |   0   |   1  |  \n",
    "| Positive |  3    |  1   |  1  |   4  |   2   |   1  |\n",
    "| Negative |  2    |  6\t  |  1  |   5  |   0   |   2  |\n",
    "| Negative |  0    |  0   |  3  |   1  |   0   |   0  |\n",
    "| Negative |  2    |  4\t  |  0  |   1  |   1   |   0  |\n",
    "\n",
    "Let's define our matrix as a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [5, 0, 2, 4, 1, 0],\n",
    "    [2, 6, 1, 2, 0, 1],\n",
    "    [3, 1, 1, 4, 2, 2],\n",
    "    [2, 6, 1, 5, 0, 2],\n",
    "    [0, 0, 3, 1, 0, 0],\n",
    "    [2, 4, 0, 1, 1, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot the variance and see how different components have different variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = []\n",
    "\n",
    "for n in range(6):\n",
    "    A_red, V, var = get_pca_reduction(A, n_components=n+1)\n",
    "    variance.append(1.0*np.var(A_red, axis=0).sum() / np.var(A, axis=0).sum())\n",
    "\n",
    "plt.plot(np.array([1.0*(n+1) for n in range(6)]), np.array(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  PCA - NLP practical example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Start from baseline\n",
    "\n",
    "We will now go back to our real example! We'll start from the same baseline as 2.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/twitter_rep_dem_data_small.csv')\n",
    "\n",
    "# Feature Extraction\n",
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=seed)\n",
    "      \n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "y_train = train_data.Party\n",
    "y_test = test_data.Party\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Baseline Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Applying PCA\n",
    "\n",
    "We will now use the scikit-learn PCA class to obtain an our reduction. Once again, go grab that coffee and take a walk, cause this might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = []\n",
    "accuracies = []\n",
    "\n",
    "dense_X_train = X_train.toarray()\n",
    "dense_X_test = X_test.toarray()\n",
    "data_var = np.var(dense_X_train, axis=0).sum()\n",
    "\n",
    "print('Total variance:')\n",
    "print(data_var)\n",
    "\n",
    "for n in [2, 10, 100, 200, 500, 1000]:\n",
    "    print(\"\\nComputing PCA for {} components\".format(str(n)))\n",
    "    pca = PCA(n_components=n, random_state=seed)\n",
    "    %timeit pca.fit(dense_X_train)\n",
    "    X_train_pca = pca.transform(dense_X_train)\n",
    "    X_test_pca = pca.transform(dense_X_test)\n",
    "    \n",
    "    print('\\nVariance:')\n",
    "    print(np.var(X_train_pca, axis=0).sum())\n",
    "    variance.append(1.0*np.var(X_train_pca, axis=0).sum() / data_var)\n",
    "    \n",
    "    clf =  KNeighborsClassifier()\n",
    "    %timeit clf.fit(X_train_pca, y_train)\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    \n",
    "    accuracies.append(accuracy_score(y_pred, y_test))\n",
    "    print('\\nAccuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([2.0, 10.0, 100.0, 200.0, 500.0, 1000.0]), np.array(true_variance))\n",
    "plt.title(\"PCA Variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([2.0, 10.0, 100.0, 200.0, 500.0, 1000.0]), np.array(accuracies))\n",
    "plt.title(\"PCA Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar to the ones obtained with SVD, which is intuitive since both methods try to obtain meaningful components that represent the data. As referred before, the use of this classifier points out the problems of high dimensionality data, and should show you that these methods can be very useful in solving that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Remarks\n",
    "\n",
    "We went through PCA and SVD on this notebook, which are methods you can try out in virtually any kind of data, whether your purpose is compression, visualization or just dimensionality reduction for classification. Some final notes:\n",
    "\n",
    "1 - You must have realized by now that the performance of the methods is tightly coupled to the classifier you choose, and that not always methods that intuitively seem to be good will help you in your tasks. This doesn't mean that the methods aren't good, simply that you need to first think of what model are you focusing on and whether you really need  or expect an improvement over your baseline.\n",
    "\n",
    "2 - You can combine methods like this with other feature selection and engineering techniques you saw before. As in many fields, trying these techniques in different scenarios will give you a better sense on when these can be helpful or when they will actually be harmful.\n",
    "\n",
    "We're finally done! Now rest a bit, go for a walk, and then dive into Part III. But before you go, I'll leave you with these questions:\n",
    "\n",
    "<img src=\"./media/matrix-questions-existencial.jpg\" width=\"700\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
